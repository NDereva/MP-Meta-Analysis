---
title: "Data Preparation"
author: "Mykola Dereva"
date: "24/06/2020"
output: github_document
---


## Importing data


```{r include=FALSE}
rm(list=ls())

library(tidyverse)
library(readr)
library(here)
library(janitor)
library(ggeasy)
library(car)
library(countrycode)
library(lubridate)

set.seed(42)
```


```{r}
data <- read_csv(here("data", "raw data", "Market Power Articles - Sheet2.csv"))
```

Lets clean the table a bit

```{r}
clean <- data %>%
  select(-starts_with("X1")) %>% # Drop technical columns (X11, X12)
  fill(Year, Authors, Title, Country, .direction = "down") %>% # Fill empty cells
  clean_names() %>% 
  rename(mp_type = type_of_mp,            # Shorten colnames
         mp_index = market_power_index,
         industry = industry_market,
         data_freq = data_frequency)

glimpse(clean)
```

#### Clean and convert approach column

```{r}
clean %>%
  group_by(approach) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```


I will filter out articles in which I am not sure

```{r}
clean <- clean %>%
  filter(approach %in% c("PTA", "GIM", "SFA")) %>%
  mutate(approach = as_factor(approach))
```


```{r}
clean %>%
  group_by(approach) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

Seems correct

#### Converting mp_type to categorical dtype

```{r}
clean %>%
  group_by(mp_type) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

Clean misspelled rows and filter only for Oligopoly and Oligopsony 

```{r}
clean <- clean %>%
  mutate(mp_type = if_else(mp_type == "Ologopoly", "Oligopoly", mp_type) ) %>%
  filter(mp_type %in% c("Oligopoly", "Oligopsony")) %>%
  mutate(mp_type = as_factor(mp_type))
```



```{r}
clean %>%
  group_by(mp_type) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

#### Clean and convert data_freq colum

```{r}
clean %>%
  group_by(data_freq) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

Seems fine


```{r}
clean %>%
  ggplot(aes(x = fct_reorder(data_freq, mp_index, length),
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.3, height = 0, alpha = 0.6, size = 2) +
  labs(x = "Industry", y="Market Power Index") +
  theme_minimal()
```




Also I'll create new column with number of observations per year
```{r}
clean <- clean %>%
  mutate(data_freq = as_factor(data_freq)) %>%
  mutate(n_obs_per_year = case_when(data_freq == "Yearly" ~ 1,
                                    data_freq == "Monthly" ~ 12,
                                    data_freq == "Quaterly" ~ 4,
                                    data_freq == "Daily" ~ 365,
                                    data_freq == "Weekly" ~ 52 ) )
```


```{r}
clean %>%
  group_by(n_obs_per_year) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```
seems correct
Most MP observations have Yearly frequency




#### Clean and transform industry column

```{r}
clean %>%
  group_by(industry) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

With this one it will be a bit more complicated since there are plenty of categories


I couldn't quickly figure out how tho check the presence of every value in vectors in the industry_fct column for categorization. 

So I will write custom function which returns TRUE if column contains any value in the vector and FALSE in not. 

```{r}
# helper function. Returns TRUE if any value in vector match string
contains_value <- function(string, vector) {
  
  string <- str_to_lower(string) %>% str_trim(side="both")
  
  matched <- FALSE
  
  for (word in vector) {
    
    if ( any(str_detect(string, word)) ) {
      matched <- TRUE
      break }
  }
  
  return(matched)
}


```


And another function which will classify industry into the given groups
Function that takes a industry string and classify it into given set of groups
function will check if  any key-words is contained in the input string

```{r}
classify_industry <- function(non_classified_str) {
  
  # define classification vectors
  
  dairy <- c("milk", "dairy", "cheese", "butter")
  meat <- c("meat", "pork", "beef", "hog", "poultry", "livestock", "cattle",
            "paultry", "pig")
  cereals <- c("wheat", "oat", "triticale", "grain", "cereal", "barley")
  oils <- c("canola", "oil", "fat")
  beverages <- c("tea", "beer", "water", "brew", "coffee", "liquor", "drink",
                 "spirit", "wine", "brandy", "cocoa", "beverage")
  tobaco <- c("tobacco", "cigar")
  fruits <- c("fruit", "banana", "horticulture")
  paper <- c("pulp", "paper", "wood", "sawlog")
  bread_flour <- c("bread", "flour")
  sugar <- c("sugar")
  
  
  classified <- case_when(
        contains_value(non_classified_str, dairy) ~ "dairy",
        contains_value(non_classified_str, meat) ~ "meat",
        contains_value(non_classified_str, cereals) ~ "cereals",
        contains_value(non_classified_str, oils) ~ "oils",
        contains_value(non_classified_str, beverages) ~ "beverages",
        contains_value(non_classified_str, tobaco) ~ "tobacco",
        contains_value(non_classified_str, fruits) ~ "fruits",
        contains_value(non_classified_str, paper) ~ "paper",
        contains_value(non_classified_str, bread_flour) ~ "bread or flour",
        contains_value(non_classified_str, sugar) ~ "sugar",
        TRUE ~ "other"
                          )
  return(classified)
}
```

Check the function results
```{r}
clean %>% 
  mutate(classified = map_chr(industry, classify_industry)) %>% 
  select(industry, classified)
```

```{r}
clean <- clean %>% 
  mutate(industry_fct = map_chr(industry, classify_industry))
```


Seems that the values were changed correctly

Lets aggregate the data in column

```{r}
clean %>%
  group_by(industry_fct) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```
Probably I shouldn't use groups with less then 10 observations. The results might be unreliable.

#### now lets convert industry_fct to factor 
and keep only groups with more then 10 observations
```{r}
clean <- clean %>%
  mutate(industry_fct = as_factor(industry_fct)) %>%
  mutate(industry_fct = fct_lump_min(industry_fct, min = 10, other_level = "other"))
```

```{r}
clean %>%
  group_by(industry_fct) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```


```{r}
clean %>%
  ggplot(aes(x = fct_reorder(industry_fct, mp_index, median),
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.25, height = 0, alpha = 0.6, size = 2) +
  labs(x = "Industry", y="Market Power Index") +
  theme_minimal()
```
Seems that there is an outlier in cereals

### Clean country column 

```{r}
clean %>%
  group_by(country) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  ungroup() %>%
  arrange(-n) %>% 
  head(10)

```

Seems that countries can be categorized into 3 main groups USA, EU and Australia
I am not sure about Australia since most of the observations are from single article.


I will use country names a lot with different data sets, so it is important to make
sure that I countries written in the same way everywhere. 

I will create small function which will standardize country names with the help of
countrycode package

```{r}
standatise_country_name <- function(old_country_name, 
                                    initial_code_scheme = "country.name") {
  
  new_country_name <- countrycode(sourcevar = old_country_name,
                        origin = initial_code_scheme,
                        destination = "country.name",
                        warn = TRUE,
                        nomatch = NULL)
  }
```

check function
```{r}
clean %>% 
  mutate(country_st = standatise_country_name(country)) %>% 
  distinct(country, country_st)
```
seems that it works well 

```{r}
clean <- clean %>% 
  mutate(country = standatise_country_name(country))
```




create country_fct column

```{r}
clean <- clean %>%
  mutate(country_fct = case_when(
              country %in% standardise_aes_names(
                #vector with European countries
                              c("Austria",	"Italy",
                                "Belgium",	"Latvia",
                                "Bulgaria",	"Lithuania",
                                "Croatia",	"Luxembourg",
                                "Cyprus",	  "Malta",
                                "Czech Republic",	"Netherlands",
                                "Denmark",	"Poland",
                                "Estonia",	"Portugal",
                                "Finland",	"Romania",
                                "France",	  "Slovakia",
                                "Germany",	"Slovenia",
                                "Greece",	  "Spain",
                                "Hungary",	"Sweden",
                                "Ireland",  "EU",
                                "Norway", "UK", "United Kingdom")
                  # all those countries will be set to europe
                                                    ) ~ "europe",
              country %in% c("United States", "Canada") ~ "n_america",
              TRUE ~ "other")
         ) %>%
  mutate(country_fct = as_factor(country_fct))
  
clean %>%
  group_by(country_fct) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```
Seems fine. 



```{r}
clean %>%
  ggplot(aes(x = fct_reorder(country_fct, mp_index, median),
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.25, height = 0, alpha = 0.6, size = 2) +
  labs(x = "Geographical Position", y="Market Power Index") +
  theme_minimal()
```


### add column with number of observations in article

```{r}
clean <- clean %>%
  separate(period, into = c("obs_start", "obs_stop"), 
           convert =TRUE, remove = FALSE) %>%
  mutate(obs_years = obs_stop - obs_start) %>%
  mutate(n_of_obs = n_obs_per_year * obs_years)
```

```{r}
clean %>%
  select(obs_start, obs_stop, obs_years, n_obs_per_year, n_of_obs) %>%
  head(10)

```
Seems correct 

check for missing data
```{r}
sum(is.na(clean$n_of_obs))

```



```{r}
clean %>%
  filter(!is.na(n_of_obs)) %>%
  ggplot(aes(x=n_of_obs, y=mp_index, color = approach)) +
  geom_point(size = 3, alpha = 0.4) +
  scale_x_continuous(trans = "log10") +
  theme_minimal() +
  easy_move_legend(to = "bottom") +
    labs(x = "Number of observations (log10)",
         y = "Market Power Index") +
  easy_add_legend_title("Approach used:")

```

### Create dummy variable after 2005
Hypothesis is that there might me a difference between in mp index between older articles and newer ones. 
The decision to choose 2005 is a bit arbitrary, but it divide data in 2 sets with approximately the same number of observations.


```{r}
clean <- clean %>%
  mutate(
          after_2005 = ifelse(
            year > 2005,
            yes = 1,
            no = 0) 
        ) 

clean %>%
  select(year, after_2005) %>%
  head(10)
```

Seems correct

```{r}
clean %>%
  ggplot(aes(x = as_factor(after_2005),
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.25, height = 0, alpha = 0.6, size = 2) +
  labs(x = "After 2005", y="Market Power Index") +
  theme_minimal()
```

### Dummy for perishable goods

```{r}
clean <- clean %>%
  mutate(
          perish = case_when(
                        map_lgl(industry, ~ contains_value(.x,
                        # if industry contains any of values below it is considered
                        # to be perishable and it will be set to 1 and 0 otherwise
                                      c("beef", "pork", "meat", "milk",
                                        "paultry", "cheese", "bakery",
                                        "poultry", "egg", "fish",
                                        "salmon", "hog", "ice", "dairy",
                                        "cake", "cattle"))) ~ 1,
                        
                                        TRUE ~ 0) )  %>%
  # set 0 to non perishable goods which mistakenly was missclassified
  mutate(perish = case_when(
                        map_lgl(industry, ~ contains_value(.x,
                        # this chunk fix the bug that cond. milk and rise were
                        # classified as perishables
                                        c("cond", "rice"))) ~ 0,
                                        TRUE ~ perish) )
```



check results 

```{r}
clean %>% 
  select(industry, perish) %>% 
  filter(perish == 1)
```
Seems correct


## Load data related to Agricultural support (PSE) and Producer protection

### Adding Agr. support (PSE) to our data

data is taken from OECD website 
[sorce](https://data.oecd.org/agrpolicy/agricultural-support.htm) 



```{r}
support <- read_csv( here("data", "raw data",
                          "Agricultural support (PSE) OESD .csv") )
```

We decided to use PSE index in our analysis
But first, rename colnames to make them easier to use. 

```{r}
support <- support %>% 
  clean_names()

glimpse(support)
```

Filter for PSE values in % of gross farm receipts

```{r}
support <- support %>% 
  filter(subject == "PSE", measure == "PC_GFARM") 

nrow(support)
```

Data shrieked significantly

From the data we need only 3 columns:
Location, year and value 
Lets drop other columns

```{r}
support <-  support %>% 
  select(location, year = time, pse = value)

head(support)
```
Seems correct 
Now we need to join PSE indicator to out main data set.
To do so, we need to unify country names across data sets.
Seems that contrycode package might really help.

```{r}
guess_field(support$location)
```
Seems that OECD uses one of beforewritten country codes
lets convert it to the regular English country names

```{r}
support$country <- map_chr(support$location, ~ standatise_country_name(.x, "genc3c"))
```


lets check the results

```{r}
support %>% 
  distinct(location, country) %>% 
  head()
```
Beautiful 

Drop columns we don't need 

```{r}
support <- support %>% 
  select(country, year, pse)
```



Since OECD data not provide PSE data related to the individual EU countries
we will have to use EU28 aggregate PSE information 

```{r}
EU28 <- standatise_country_name( c(
                      "Austria",	"Italy",
                      "Belgium",	"Latvia",
                      "Bulgaria",	"Lithuania",
                      "Croatia",	"Luxembourg",
                      "Cyprus",	  "Malta",
                      "Czech Republic",	"Netherlands",
                      "Denmark",	"Poland",
                      "Estonia",	"Portugal",
                      "Finland",	"Romania",
                      "France",	  "Slovakia",
                      "Germany",	"Slovenia",
                      "Greece",	  "Spain",
                      "Hungary",	"Sweden",
                      "Ireland", "United Kingdom"
                                    )
                                )
```

Lets estimate average PSE related to each MPI observation 

Fist of all let's make a column which contain all the years during which 
each MPI was estimated
```{r}
country_year <- clean %>%
  select(country, obs_start, obs_stop) %>%
  # create unique identification of each MPI
  tibble::rowid_to_column("id") %>%   
  # transform start and end date to one column 
  gather(key = "start_stop", value = "year", obs_start:obs_stop) %>% 
  arrange(id) %>% 
  # convert this new column as date
  mutate(year = as.Date(as.character(year), format = "%Y")) %>% 
  group_by(id) %>% 
  # fill sequence of of years
  complete(year = seq(min(year), max(year), by ="year")) %>% 
  ungroup() %>% 
  #drop start_stop columns since we don't need it
  select(id, country, year) %>% 
  # fill empty values
  fill(country, .direction = "down") %>%
  # convert datetype to keep just year information
  # and drop moth and day
  mutate(year = year(year))

country_year %>% head()
```

Now we can easily join country_year with PSE data to calculate the average
PSE per MPI estimate
```{r}
mean_pse <- country_year %>% 
  # change EU country names to match OECD aggregation 
  # OECD data
  mutate(country = case_when(
                        country %in% EU28 ~ "EU28",
                        TRUE ~ country)) %>% 
  # join information related to PSE index
  left_join(support, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average PSE index attributed to each MPI observation
  group_by(id) %>% 
  summarise(mean_pse = mean(pse, na.rm=TRUE)) %>% 
  ungroup()

head(mean_pse)
```



```{r}
mean(is.na(mean_pse$mean_pse))

```
Unfortunately we 4.5% of observations is missing
Probably this is because many articles use too old data for the MPI estimation
While PSE index data is available starting from 1986,



```{r}
clean <- clean %>% 
  # create an id for each of MPI observation to join it to PSE data
  tibble::rowid_to_column("id") %>% 
  inner_join(mean_pse, by = "id", copy = TRUE)
```
Now we have PSE information in out data set


Plot the resulting variable 
```{r}
ggplot(clean, aes(mean_pse, mp_index, color=approach)) +
  geom_point(alpha = 0.7, size = 5, shape = 1) +
  theme_minimal()
```


Check observations with negative PSE
```{r}
clean %>% 
  filter(!is.na(mean_pse), mean_pse < 0) %>% 
  select(country, id, mean_pse) %>% 
  mutate(log_mean_pse = log(mean_pse))
```
Ukraine ¯\_(ツ)_/¯


### Adding Producer Protection (PP) index to our data


The algorithm will be the similar as in case of PSE
so I will not comment it

Data source is the same 
[sorce](https://data.oecd.org/agrpolicy/producer-protection.htm#indicator-chart) 

```{r}
PP <- read_csv( here("data", "raw data",
                     "Producer Protection (PP) OECD.csv") )
```


```{r}
PP <- PP %>% 
  clean_names()

glimpse(PP)
```

```{r}
PP <-  PP %>% 
  select(location, year = time, PP = value)

head(PP)
```


```{r}
PP$country <- map_chr(PP$location, ~ standatise_country_name(.x, "genc3c"))

PP %>% 
  distinct(location, country) %>% 
  head()
```


```{r}
PP <- PP %>% 
  select(country, year, PP)

```



```{r}
mean_pp <- country_year %>%
  mutate(country = case_when(
                      country %in% EU28 ~ "EU28",
                      TRUE ~ country)) %>% 
  left_join(PP, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average PSE index attributed to each MPI observation
  group_by(id) %>% 
  summarise(mean_pp = mean(PP, na.rm=TRUE)) %>% 
  ungroup()

head(mean_pp)
```

```{r}
mean(is.na(mean_pp))
```
Luckily there is only 2.2% of missing values in mean PP indicator

```{r}
clean <- clean %>% 
  inner_join(mean_pp, by = "id", copy = TRUE)
```




```{r}
ggplot(clean, aes(mean_pp, mp_index, color=approach)) +
  geom_point(alpha = 0.7, size = 5, shape = 1) +
  theme_minimal()
```
Maybe I should not use those 4 outliers in the analysis


## Small Farms Share

Lets add variable witch contains information related to the market share of small 
farmers

The EU data is taken from the Eurostat website 
The code of the data set is "ef_kvftaa"
In the our analysis we define small farmers as ones who have less than 20 ha of 
land. Small farmers variable will contain the share of small farmers output (in EUR)
in total agricultural output. 


MAYBE IT IS BETTER TO USE INDUSTRY DATA, NOT COUNTRY AGGREGATE

```{r}
farms <- read_csv(here("data", "raw data", "Farm structure EU",
                       "ef_kvftaa_1_Data.csv"),
                  na = c("", "NA", ":") # NA is coded as ":"
                  )
```


drop columns we don't need and rename ones we need

```{r}
farms <- farms %>% 
  clean_names() %>% 
  select(country = geo, agrarea, year = time, output_eur = value)

glimpse(farms)
```


Standardize country names

```{r}
guess_field(farms$country) %>% head(3)
```

```{r}
farms$country <- standatise_country_name(farms$country)
```


check the levels of farm size aggregation
```{r}
farms %>% 
  pivot_wider(names_from = agrarea, 
              values_from = output_eur) %>% 
  clean_names() %>% 
  colnames()
```


create column with share of small farms
Also we will assume that in the 2000 year small farm market share == 2005
and 2018 == 2013 to alleviate issue of sample size reduction. 

```{r}
sfarm_share <- farms %>%
  pivot_wider(names_from = agrarea, 
              values_from = output_eur) %>% 
  group_by(country, year) %>%
  clean_names() %>% 
  mutate(sfarms = sum(zero_ha, less_than_2_ha, from_2_to_4_9_ha,
                        from_5_to_9_9_ha, from_10_to_19_9_ha, na.rm = TRUE),
         sfarms_share = sfarms / total * 100) %>% 
  ungroup() %>% 
  # add data for 2000 and 2018
  select(country, year, sfarms_share) %>% 
  pivot_wider(names_from = year, values_from = sfarms_share) %>%
  mutate(`2000` = `2005`, `2018` = `2013`, `1995` = `2005`) %>% 
  pivot_longer(cols = -country,
               names_to = "year",
               values_to = "sfarm_share") %>% 
  mutate(year = as.numeric(year))

head(sfarm_share)
```



join sfarm_share with main data set



```{r}
sfarm <-country_year %>%
  left_join(sfarm_share, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average PSE index attributed to each MPI observation
  group_by(id) %>% 
  summarise(sfarm_share = mean(sfarm_share, na.rm=TRUE)) %>% 
  ungroup()

head(sfarm)

```


```{r}
sum(is.na(sfarm))
```
A lot of missing observations


```{r}
clean <- clean %>% 
  inner_join(sfarm, by = "id", copy = TRUE)
```




```{r}
ggplot(clean, aes(sfarm_share, mp_index, color=approach)) +
  geom_point(alpha = 0.4, size = 5) +
  theme_minimal()
```

## The same data but aggregated by industry



```{r warning=FALSE}

farms_full <- read_csv(here("data", "raw data", "Farm structure EU",
                       "ef_kvftaa_1_Data_full.csv"),
                  na = c("", "NA", ":") # NA is coded as ":"
                  )
```


drop columns we don't need and rename ones we need

```{r}
farms_full <- farms_full %>% 
  clean_names() 

farms_full %>% glimpse()
```

The data set is a bit redundant
filter only information we need

```{r}
farms_full <- farms_full %>% 
  filter(indic_ef == "Euro: Standard output (SO)",
         farmtype != "Total") %>% 
  select(-flag_and_footnotes, -indic_ef) %>% 
  select(country = geo, agrarea, industry = farmtype,
         year = time, output_eur = value)

farms_full %>% glimpse()
```


We need to categorise industry to match the main dataset
```{r}
farms_full %>% 
  distinct(industry) 
```




remove "(calculated with Standard Output)"
```{r}
farms_full <- farms_full %>% 
  mutate(industry = str_remove(industry, pattern = " \\(.+\\)")) 
```

#### classify industry
Try to classify with the function we already created
```{r}
farms_ind_rename <- farms_full %>% 
  select(industry) %>% 
  distinct() %>%
  mutate(new_value = map_chr(industry, ~ classify_industry(.x))) %>% 
  select(old_value = industry, new_value)

farms_ind_rename
```
Classified satisfactory


It will not be very efficient to categories each of 28k values.
Lets write function which will map predefined value pairs 

```{r}

change_string_value <- function(target_value, old_values, new_values) {
  
  
  result <- NULL
  
  for (i in 1:length(old_values) ) {
    
    if (target_value == old_values[i]) {
      
      result <- new_values[i]
      
      break }
    
  }
  
  # if result is still NULL throw an error
  if ( is.null(result) ) stop("No matches, check old_values")
  
  return(result)
}

```


check function
```{r}
farms_full %>% 
  select(industry) %>% 
  mutate(industry_cat = map_chr(farms_full$industry,
                                ~ change_string_value(target_value = .x,
                    old_values = farms_ind_rename$old_value,
                    new_values = farms_ind_rename$new_value))) %>%
  head(10)
```
Work well








classification function we created before works satisfactory


```{r}
farms_full <- farms_full %>% 
  mutate(industry_cat = map_chr(farms_full$industry,
                                ~ change_string_value(target_value = .x,
                    old_values = farms_ind_rename$old_value,
                    new_values = farms_ind_rename$new_value))) %>% 
  select(-industry)
```



#### standardize country name
```{r}
farm_country_rename <- farms_full %>% 
  select(old_country = country) %>% 
  distinct() %>% 
  mutate(new_country = map_chr(old_country, ~ standatise_country_name(.x)))

head(farm_country_rename)
```

```{r}
farms_full <- farms_full %>% 
  mutate(country = map_chr(country,
                                ~ change_string_value(target_value = .x,
                                old_values = farm_country_rename$old_country,
                                new_values = farm_country_rename$new_country)))
```



#### change column position
```{r}
farms_full <- farms_full %>% 
  select(country, industry_cat, year, everything())
```




### prepare for joining
```{r}
farms_full_to_join <- farms_full %>%
  pivot_wider(names_from = agrarea, 
              values_from = output_eur,
              values_fn = sum) %>%      # to have a number in cell, it's list otherwise
  clean_names() %>% 
  group_by(country, year, industry_cat) %>%
  mutate(sfarms = sum(zero_ha, less_than_2_ha, from_2_to_4_9_ha,
                        from_5_to_9_9_ha, from_10_to_19_9_ha, na.rm = TRUE),
         sfarms_share = sfarms / total * 100) %>% 
  # add data for 2000 and 2018
  select(country,industry_cat, year, sfarms_share) %>% 
  pivot_wider(names_from = year, values_from = sfarms_share) %>%
  mutate(`2000` = `2005`, `2018` = `2013`, `1995` = `2005`) %>% 
  pivot_longer(cols = `2005`:`1995`,
               names_to = "year",
               values_to = "sfarm_share") %>% 
  mutate(year = as.numeric(year)) %>% 
  filter(!is.na(sfarm_share))

head(farms_full_to_join)
```



```{r}
ggplot(farms_full_to_join, aes(sfarm_share) ) +
  geom_density() +
  facet_grid(rows = vars(industry_cat))

```

I didn`t keep full list of industry categories so I will create it once again
```{r}
clean$industry_cat <- map_chr(clean$industry, ~ classify_industry(.x))
```



```{r}
country_ind_year <-  country_year %>% 
  left_join(clean[c("id", "industry_cat")], by = "id" ) %>% 
  select(id, country, industry_cat, year)

head(country_ind_year)
```

### 
```{r}
sfarm_by_cat <-country_ind_year %>%
  left_join(farms_full_to_join, 
            by = c("country", "industry_cat", "year"),
            copy = TRUE) %>% 
  # calculate an average PSE index attributed to each MPI observation
  group_by(id) %>% 
  summarise(sfarm_share_by_cat = mean(sfarm_share, na.rm=TRUE)) %>% 
  ungroup()

head(sfarm_by_cat)
```

### Finally join to main data set 

```{r}
clean <- clean %>% 
  inner_join(sfarm_by_cat, by = "id", copy = TRUE)
```


```{r}
sum(is.na(clean$sfarm_share_by_cat))
```


```{r}
ggplot(clean, aes(sfarm_share_by_cat, mp_index, color=approach)) +
  geom_point(alpha = 0.4, size = 5) +
  theme_minimal()
```


## Add data from Doing Buisiness "easines to start business"



```{r}
db <- readxl::read_excel(path = here("data", "raw data", "Doing Business.xlsx"),
                         sheet = 1, skip = 3)

dim(db)
```

select only columns we need

```{r}
db <- db %>%
  clean_names() %>% 
  select(country_code, year = db_year, score_starting_a_business)
```

```{r}
head(db, 10)
```


To decrease the number of dropped observations we assume that there is
significant difference between 2004 and 2000 in Start Business index
```{r}
db <- db %>% 
  pivot_wider(names_from = year, 
              values_from = score_starting_a_business) %>% 
  mutate(`2000` = `2004`) %>% 
  pivot_longer(cols = -country_code,
               names_to = "year",
               values_to = "score_starting_a_business") %>% 
  mutate(year = as.numeric(year))
```



Adjust country names

```{r}
guess_field(db$country_code)
```

```{r}
db$country <- countrycode(sourcevar = db$country_code,
                    origin = "genc3c",
                    destination = "country.name",
                    warn = TRUE,
                    nomatch = NULL)
```


```{r}
db %>% 
  select(country_code, country) %>% 
  distinct() %>% 
  head(10)
```

```{r}
db_join <- country_year %>%
  left_join(db, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average DB index attributed to each MPI observation
  group_by(id) %>% 
  summarise(start_business = mean(score_starting_a_business, na.rm=TRUE)) %>% 
  ungroup()

head(db_join, 10)
```

```{r}
sum(is.na(db_join$start_business))
```


```{r}
clean <- clean %>% 
  inner_join(db_join, by = "id", copy = TRUE)
```


```{r}
ggplot(clean, aes(start_business, mp_index, color=approach)) +
  geom_point(alpha = 0.4, size = 5) +
  theme_minimal()
```

## Ease of doing business index 

The algorithm is the same as with previous variable

```{r}
db <- readxl::read_excel(path = here("data", "raw data", "Doing Business.xlsx"),
                         sheet = 1, skip = 3)
```


```{r}
db <- db %>%
  clean_names() %>% 
  select(country_code, year = db_year,
         starts_with("ease_of_doing_business_score")) %>% 
  group_by(country_code, year) %>% 
  # make one column with ease of doing business index
  # assume that there is no difference in methodologies
  mutate(edb_index = sum(ease_of_doing_business_score_db17_20_methodology,
                          ease_of_doing_business_score_db15_methodology,
                          ease_of_doing_business_score_db10_14_methodology,
                          na.rm = TRUE),
         country = standatise_country_name(country_code,
                                           initial_code_scheme = "genc3c")) %>%
  ungroup() %>% 
  select(country, year, edb_index) %>%
  # because on NAs some values are == 0
  # filter them out
  filter(edb_index > 0) %>% 
  # assume that there is no big difference in doing business in 2010 and 2005
  pivot_wider(names_from = year, 
            values_from = edb_index) %>% 
  mutate(`2005` = `2010`) %>% 
  pivot_longer(cols = -country,
             names_to = "year",
             values_to = "edb_index") %>% 
  mutate(year = as.numeric(year))

head(db)
```



```{r}
edb_join <- country_year %>%
  left_join(db, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average DB index attributed to each MPI observation
  group_by(id) %>% 
  summarise(edb_index = mean(edb_index, na.rm=TRUE)) %>% 
  ungroup()

```


```{r}
clean <- clean %>% 
  inner_join(edb_join, by = "id", copy = TRUE)
```


```{r}
ggplot(clean, aes(edb_index, mp_index, color=approach)) +
  geom_point(alpha = 0.4, size = 5) +
  theme_minimal()
```

## Make an object only with columns we need for the further analysis 

```{r}
drop_col <- c("id", "year", "authors", "title", "country", "industry", "period",
              "obs_start", "obs_stop", "n_obs_per_year",
              "obs_years", "industry_cat")

analysis <- clean %>%
  # select all except of drop_col
  select(-all_of(drop_col)) %>% 
  rename(app = approach,
         freq = data_freq,
         type = mp_type,
         index = mp_index,
         ind = industry_fct,
         count = country_fct,
         obs_n = n_of_obs) %>%
  # create dummies from category columns
  fastDummies::dummy_columns(select_columns = c("freq", "type", "ind",
                                                "count", "app"),
                remove_selected_columns = TRUE)
```




```{r}
saveRDS(clean, file = here("data", "clean data", "Full_data.Rds"))
saveRDS(analysis, file = here("data", "clean data", "Analysis_data.Rds"))
```


