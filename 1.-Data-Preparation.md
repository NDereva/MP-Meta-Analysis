Data Preparation
================
Mykola Dereva
24/06/2020

## Importing data

``` r
path <- here("data", "raw data", "Market Power Articles - Sheet2.csv")
data <- read_csv(path)
```

    ## Warning: Missing column names filled in: 'X11' [11], 'X12' [12]

    ## Parsed with column specification:
    ## cols(
    ##   Year = col_double(),
    ##   Authors = col_character(),
    ##   Title = col_character(),
    ##   Country = col_character(),
    ##   `Industry/Market` = col_character(),
    ##   Approach = col_character(),
    ##   Period = col_character(),
    ##   `Data Frequency` = col_character(),
    ##   `Type of MP` = col_character(),
    ##   `Market Power Index` = col_double(),
    ##   X11 = col_character(),
    ##   X12 = col_character()
    ## )

Lets clean the table a bit

``` r
clean <- data %>%
  select(-starts_with("X1")) %>% # Drop technical columns (X11, X12)
  fill(Year, Authors, Title, Country, .direction = "down") %>% # Fill empty cells
  clean_names() %>% 
  rename(mp_type = type_of_mp,            # Shorten colnames
         mp_index = market_power_index,
         industry = industry_market,
         data_freq = data_frequency)

glimpse(clean)
```

    ## Rows: 244
    ## Columns: 10
    ## $ year      <dbl> 2008, 2008, 2008, 2008, 1982, 1982, 1997, 1990, 1990, 200...
    ## $ authors   <chr> "Anders, S. M", "Anders, S. M", "Anders, S. M", "Anders, ...
    ## $ title     <chr> "Imperfect Competition in German Food Retailing: Evidence...
    ## $ country   <chr> "Germany", "Germany", "Germany", "Germany", "USA", "USA",...
    ## $ industry  <chr> "Beef", "Beef", "Pork", "Pork", "Textile", "Tobacco", "Be...
    ## $ approach  <chr> "GIM", "GIM", "GIM", "GIM", "PTA", "PTA", "PTA", "PTA", "...
    ## $ period    <chr> "1995-2000", "1995-2000", "1995-2000", "1995-2000", "1947...
    ## $ data_freq <chr> "Monthly", "Monthly", "Monthly", "Monthly", "Yearly", "Ye...
    ## $ mp_type   <chr> "Oligopsony", "Oligopoly", "Oligopsony", "Oligopoly", "Ol...
    ## $ mp_index  <dbl> 0.17600, 0.08900, 0.01100, 0.00300, 0.03684, 0.40190, -0....

#### Clean and convert approach column

``` r
clean %>%
  group_by(approach) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

    ## # A tibble: 5 x 3
    ##   approach     n  prop
    ##   <chr>    <int> <dbl>
    ## 1 PTA        108 44.3 
    ## 2 GIM         73 29.9 
    ## 3 SFA         55 22.5 
    ## 4 Other ?      6  2.46
    ## 5 ?            2  0.82

I will filter out articles in which I am not sure

``` r
clean <- clean %>%
  filter(approach %in% c("PTA", "GIM", "SFA")) %>%
  mutate(approach = as_factor(approach))
```

``` r
clean %>%
  group_by(approach) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

    ## # A tibble: 3 x 3
    ##   approach     n  prop
    ##   <fct>    <int> <dbl>
    ## 1 PTA        108  45.8
    ## 2 GIM         73  30.9
    ## 3 SFA         55  23.3

Seems correct

#### Converting mp\_type to categorical dtype

``` r
clean %>%
  group_by(mp_type) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

    ## # A tibble: 4 x 3
    ##   mp_type                    n  prop
    ##   <chr>                  <int> <dbl>
    ## 1 Oligopoly                152 64.4 
    ## 2 Oligopsony                68 28.8 
    ## 3 Conjectural elasticity     9  3.81
    ## 4 Conjectural variation      7  2.97

Clean misspelled rows and filter only for Oligopoly and Oligopsony

``` r
clean <- clean %>%
  mutate(mp_type = if_else(mp_type == "Ologopoly", "Oligopoly", mp_type) ) %>%
  filter(mp_type %in% c("Oligopoly", "Oligopsony")) %>%
  mutate(mp_type = as_factor(mp_type))
```

``` r
clean %>%
  group_by(mp_type) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

    ## # A tibble: 2 x 3
    ##   mp_type        n  prop
    ##   <fct>      <int> <dbl>
    ## 1 Oligopoly    152  69.1
    ## 2 Oligopsony    68  30.9

#### Clean and convert data\_freq colum

``` r
clean %>%
  group_by(data_freq) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

    ## # A tibble: 4 x 3
    ##   data_freq     n  prop
    ##   <chr>     <int> <dbl>
    ## 1 Yearly      188 85.4 
    ## 2 Monthly      25 11.4 
    ## 3 Quaterly      6  2.73
    ## 4 Weekly        1  0.45

Seems fine

``` r
clean %>%
  ggplot(aes(x = fct_reorder(data_freq, mp_index, length),
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.3, height = 0, alpha = 0.6, size = 2) +
  labs(x = "Industry", y="Market Power Index") +
  theme_minimal()
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->

Also I’ll create new column with number of observations per year

``` r
clean <- clean %>%
  mutate(data_freq = as_factor(data_freq)) %>%
  mutate(n_obs_per_year = case_when(data_freq == "Yearly" ~ 1,
                                    data_freq == "Monthly" ~ 12,
                                    data_freq == "Quaterly" ~ 4,
                                    data_freq == "Daily" ~ 365,
                                    data_freq == "Weekly" ~ 52 ) )
```

``` r
clean %>%
  group_by(n_obs_per_year) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

    ## # A tibble: 4 x 3
    ##   n_obs_per_year     n  prop
    ##            <dbl> <int> <dbl>
    ## 1              1   188 85.4 
    ## 2             12    25 11.4 
    ## 3              4     6  2.73
    ## 4             52     1  0.45

seems correct Most MP observations have Yearly frequency

#### Clean and transform industry column

``` r
clean %>%
  group_by(industry) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

    ## # A tibble: 122 x 3
    ##    industry         n  prop
    ##    <chr>        <int> <dbl>
    ##  1 Dairy           38 17.3 
    ##  2 Milk            10  4.55
    ##  3 Tea              6  2.73
    ##  4 Barley           5  2.27
    ##  5 Canola           5  2.27
    ##  6 Wheat            5  2.27
    ##  7 Oats             4  1.82
    ##  8 Triticale        4  1.82
    ##  9 Cereal foods     3  1.36
    ## 10 Cheese           3  1.36
    ## # ... with 112 more rows

With this one it will be a bit more complicated since there are plenty
of categories

clean the column a bit

``` r
clean <- clean %>%
#  filter(!is.na(industry)) %>%                        # drop N/A
  mutate(industry_ctg = str_to_lower(industry) %>%    # to lowercase
                          str_trim(side="both") ) 
```

``` r
clean %>%
  select(industry, industry_ctg)
```

    ## # A tibble: 220 x 2
    ##    industry  industry_ctg
    ##    <chr>     <chr>       
    ##  1 Beef      beef        
    ##  2 Beef      beef        
    ##  3 Pork      pork        
    ##  4 Pork      pork        
    ##  5 Textile   textile     
    ##  6 Tobacco   tobacco     
    ##  7 Meat      meat        
    ##  8 Livestock livestock   
    ##  9 Hogs      hogs        
    ## 10 Hogs      hogs        
    ## # ... with 210 more rows

create vectors which contain key words for further grouping

``` r
dairy <- c("milk", "dairy", "cheese", "butter")
meat <- c("meat", "pork", "beef", "hog", "poultry", "livestock", "cattle")
cereals <- c("wheat", "oat", "triticale", "grain", "cereal", "barley")
oils <- c("canola", "oil", "fat")
beverages <- c("tea", "beer", "water", "brew", "coffee", "liquor", "drink",
               "spirit", "wine", "brandy", "cocoa", "beverage")
tobaco <- c("tobacco", "cigar")
fruits <- c("fruit", "banana")
paper <- c("pulp", "paper", "wood", "sawlog")
bread_flour <- c("bread", "flour")
sugar <- c("sugar")
```

I couldn’t quickly figure out how tho check the presence of every value
in vectors in the industry\_fct column for categorization.

So I will write custom function which returns TRUE if column contains
any value in the vector and FALSE in not.

``` r
# helper function. Returns TRUE if any value in vector match string
contains_value <- function(string, vector) {
  
  matched <- FALSE
  
  for (word in vector) {
    
    if ( any(str_detect(string, word)) ) {
      matched <- TRUE
      break }
  }
  
  return(matched)
}
```

Test the function

``` r
# Function returns TRUE for each row which have any matches with vector

col_contain_val <- function(column, vector){
  x <- c()
  
  for (i in 1:length(column) ) {
    x[i] <- contains_value(column[i], vector)
  }
  
  return(x)
}

#test if any value in meat column is present in industr_fct column
print(clean$industry_ctg[1:7])
```

    ## [1] "beef"    "beef"    "pork"    "pork"    "textile" "tobacco" "meat"

``` r
col_contain_val(clean$industry_ctg[1:7], meat)
```

    ## [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE

Seems like it works fine

``` r
clean <- clean %>%
  mutate(industry_fct = case_when(
                          col_contain_val(industry_ctg, dairy) ~ "dairy",
                          col_contain_val(industry_ctg, meat) ~ "meat",
                          col_contain_val(industry_ctg, cereals) ~ "cereals",
                          col_contain_val(industry_ctg, oils) ~ "oils",
                          col_contain_val(industry_ctg, beverages) ~ "beverages",
                          col_contain_val(industry_ctg, tobaco) ~ "tobacco",
                          col_contain_val(industry_ctg, fruits) ~ "fruits",
                          col_contain_val(industry_ctg, paper) ~ "paper",
                          col_contain_val(industry_ctg, bread_flour) ~ "bread or flour",
                          col_contain_val(industry_ctg, sugar) ~ "sugar",
                          TRUE ~ industry_ctg
                                )
         )

clean %>%
  select(industry, industry_fct)
```

    ## # A tibble: 220 x 2
    ##    industry  industry_fct
    ##    <chr>     <chr>       
    ##  1 Beef      meat        
    ##  2 Beef      meat        
    ##  3 Pork      meat        
    ##  4 Pork      meat        
    ##  5 Textile   textile     
    ##  6 Tobacco   tobacco     
    ##  7 Meat      meat        
    ##  8 Livestock meat        
    ##  9 Hogs      meat        
    ## 10 Hogs      meat        
    ## # ... with 210 more rows

Seems that the values were changed correctly

Lets aggregate the data in column

``` r
clean %>%
  group_by(industry_fct) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

    ## # A tibble: 49 x 3
    ##    industry_fct       n  prop
    ##    <chr>          <int> <dbl>
    ##  1 dairy             61 27.7 
    ##  2 cereals           28 12.7 
    ##  3 beverages         24 10.9 
    ##  4 meat              20  9.09
    ##  5 oils              11  5   
    ##  6 paper              9  4.09
    ##  7 bread or flour     7  3.18
    ##  8 fruits             7  3.18
    ##  9 tobacco            6  2.73
    ## 10 sugar              5  2.27
    ## # ... with 39 more rows

Probably I shouldn’t use groups with less then 10 observations. The
results might be unreliable.

#### now lets convert industry\_fct to factor

``` r
clean <- clean %>%
  mutate(industry_fct = as_factor(industry_fct)) %>%
  mutate(industry_fct = fct_lump_min(industry_fct, min = 10, other_level = "other"))
```

``` r
clean %>%
  group_by(industry_fct) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

    ## # A tibble: 6 x 3
    ##   industry_fct     n  prop
    ##   <fct>        <int> <dbl>
    ## 1 other           76 34.6 
    ## 2 dairy           61 27.7 
    ## 3 cereals         28 12.7 
    ## 4 beverages       24 10.9 
    ## 5 meat            20  9.09
    ## 6 oils            11  5

``` r
clean %>%
  ggplot(aes(x = fct_reorder(industry_fct, mp_index, median),
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.25, height = 0, alpha = 0.6, size = 2) +
  labs(x = "Industry", y="Market Power Index") +
  theme_minimal()
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-24-1.png)<!-- -->
Seems that there is an outlier in cereals

### Clean country column

``` r
clean %>%
  group_by(country) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  ungroup() %>%
  arrange(-n) %>% 
  head(10)
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

    ## # A tibble: 10 x 3
    ##    country       n  prop
    ##    <chr>     <int> <dbl>
    ##  1 USA          96 43.6 
    ##  2 Australia    40 18.2 
    ##  3 Spain        15  6.82
    ##  4 Ukraine      10  4.55
    ##  5 Germany       6  2.73
    ##  6 Hungary       5  2.27
    ##  7 Austria       3  1.36
    ##  8 Brazil        3  1.36
    ##  9 Canada        2  0.91
    ## 10 Estonia       2  0.91

Seems that countries can be categorized into 3 main groups USA, EU and
Australia I am not sure about Australia since most of the observations
are from single article.

``` r
# Vector with all of the Europe countries 

europe <- c(
        "Austria",  "Italy",
        "Belgium",  "Latvia",
        "Bulgaria", "Lithuania",
        "Croatia",  "Luxembourg",
        "Cyprus",     "Malta",
        "Czech Republic",   "Netherlands",
        "Denmark",  "Poland",
        "Estonia",  "Portugal",
        "Finland",  "Romania",
        "France",     "Slovakia",
        "Germany",  "Slovenia",
        "Greece",     "Spain",
        "Hungary",  "Sweden",
        "Ireland",  "EU",
        "Norway", "UK", "United Kingdom"
        )

developing <- c("Ukraine", "Brazil", "China", "Haiti", "India",
                "Kenya", "Sri Lanka")
```

Lets also standardize country names

``` r
clean$country_st <-  countrycode(sourcevar = clean$country,
                        origin = "country.name",
                        destination = "country.name",
                        warn = TRUE,
                        nomatch = NULL)
```

Check the result

``` r
clean %>% 
  distinct(country, country_st) %>% head()
```

    ## # A tibble: 6 x 2
    ##   country     country_st   
    ##   <chr>       <chr>        
    ## 1 Germany     Germany      
    ## 2 USA         United States
    ## 3 Hungary     Hungary      
    ## 4 Sweden      Sweden       
    ## 5 Netherlands Netherlands  
    ## 6 Canada      Canada

Beautiful

create country\_fct column

``` r
clean <- clean %>%
  mutate(country_fct = case_when(
                        country_st %in% europe ~ "europe",
                        col_contain_val(country_st, c("United States",
                                                      "Canada")) ~ "n_america",
                        #col_contain_val(country_st, developing) ~ "developing",
                        #col_contain_val(country, "Australia") ~ "Australia",
                        TRUE ~ "other")
         ) %>%
  mutate(country_fct = as_factor(country_fct))
  
clean %>%
  group_by(country_fct) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

    ## # A tibble: 3 x 3
    ##   country_fct     n  prop
    ##   <fct>       <int> <dbl>
    ## 1 n_america     100  45.4
    ## 2 other          61  27.7
    ## 3 europe         59  26.8

Seems fine.

``` r
clean %>%
  ggplot(aes(x = fct_reorder(country_fct, mp_index, median),
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.25, height = 0, alpha = 0.6, size = 2) +
  labs(x = "Geographical Position", y="Market Power Index") +
  theme_minimal()
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-30-1.png)<!-- -->

### add column with number of observations in article

``` r
clean <- clean %>%
  separate(period, into = c("obs_start", "obs_stop"), 
           convert =TRUE, remove = FALSE) %>%
  mutate(obs_years = obs_stop - obs_start) %>%
  mutate(n_of_obs = n_obs_per_year * obs_years)
```

``` r
clean %>%
  select(obs_start, obs_stop, obs_years, n_obs_per_year, n_of_obs) %>%
  head(10)
```

    ## # A tibble: 10 x 5
    ##    obs_start obs_stop obs_years n_obs_per_year n_of_obs
    ##        <int>    <int>     <int>          <dbl>    <dbl>
    ##  1      1995     2000         5             12       60
    ##  2      1995     2000         5             12       60
    ##  3      1995     2000         5             12       60
    ##  4      1995     2000         5             12       60
    ##  5      1947     1971        24              1       24
    ##  6      1947     1971        24              1       24
    ##  7      1959     1982        23              1       23
    ##  8      1959     1982        23              1       23
    ##  9      1993     2003        10             12      120
    ## 10      1995     2004         9             12      108

Seems correct

check for missing data

``` r
sum(is.na(clean$n_of_obs))
```

    ## [1] 0

``` r
clean %>%
  filter(!is.na(n_of_obs)) %>%
  ggplot(aes(x=n_of_obs, y=mp_index, color = approach)) +
  geom_point(size = 3, alpha = 0.4) +
  scale_x_continuous(trans = "log10") +
  theme_minimal() +
  easy_move_legend(to = "bottom") +
    labs(x = "Number of observations (log10)",
         y = "Market Power Index") +
  easy_add_legend_title("Approach used:")
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-34-1.png)<!-- -->

### Create dummy variable after 2005

Hypothesis is that there might me a difference between in mp index
between older articles and newer ones. The decision to choose 2005 is a
bit arbitrary, but it divide data in 2 sets with approximately the same
number of observations.

``` r
clean <- clean %>%
  mutate(
          after_2005 = ifelse(
            year > 2005,
            yes = 1,
            no = 0) 
        ) 

clean %>%
  select(year, after_2005) %>%
  head(10)
```

    ## # A tibble: 10 x 2
    ##     year after_2005
    ##    <dbl>      <dbl>
    ##  1  2008          1
    ##  2  2008          1
    ##  3  2008          1
    ##  4  2008          1
    ##  5  1982          0
    ##  6  1982          0
    ##  7  1990          0
    ##  8  1990          0
    ##  9  2009          1
    ## 10  2009          1

Seems correct

``` r
clean %>%
  ggplot(aes(x = as_factor(after_2005),
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.25, height = 0, alpha = 0.6, size = 2) +
  labs(x = "After 2005", y="Market Power Index") +
  theme_minimal()
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-36-1.png)<!-- -->

### Dummy for perishable goods

``` r
perishables <- c("beef", "pork", "meat", "milk", "paultry", "cheese", "bakery",
                 "poultry", "egg", "fish", "salmon", "hog", "ice", "dairy",
                 "cake", "cattle")

clean <- clean %>%
  mutate(
          perish = case_when(
                        col_contain_val(industry_ctg,
                                        perishables) ~ 1,
                                        TRUE ~ 0) )  %>%
  # set 0 to non perishable goods which mistakenly was missclassified
  mutate(perish = case_when(
                        col_contain_val(industry_ctg,
                                        c("cond", "rice")) ~ 0,
                                        TRUE ~ perish) )
```

check results

``` r
clean %>% 
  select(industry, perish) %>% 
  filter(perish == 1)
```

    ## # A tibble: 88 x 2
    ##    industry       perish
    ##    <chr>           <dbl>
    ##  1 Beef                1
    ##  2 Beef                1
    ##  3 Pork                1
    ##  4 Pork                1
    ##  5 Meat                1
    ##  6 Hogs                1
    ##  7 Hogs                1
    ##  8 Meat Packing        1
    ##  9 Paultry & Eggs      1
    ## 10 Cheese              1
    ## # ... with 78 more rows

## Load data related to Agricultural support (PSE) and Producer protection

### Adding Agr. support (PSE) to our data

data is taken from OECD website
[sorce](https://data.oecd.org/agrpolicy/agricultural-support.htm)

``` r
support <- read_csv( here("data", "raw data",
                          "Agricultural support (PSE) OESD .csv") )
```

    ## Parsed with column specification:
    ## cols(
    ##   LOCATION = col_character(),
    ##   INDICATOR = col_character(),
    ##   SUBJECT = col_character(),
    ##   MEASURE = col_character(),
    ##   FREQUENCY = col_character(),
    ##   TIME = col_double(),
    ##   Value = col_double(),
    ##   `Flag Codes` = col_logical()
    ## )

We decided to use PSE index in our analysis But first, rename colnames
to make them easier to use.

``` r
support <- support %>% 
  clean_names()

glimpse(support)
```

    ## Rows: 8,848
    ## Columns: 8
    ## $ location   <chr> "AUS", "AUS", "AUS", "AUS", "AUS", "AUS", "AUS", "AUS", ...
    ## $ indicator  <chr> "AGRSUPP", "AGRSUPP", "AGRSUPP", "AGRSUPP", "AGRSUPP", "...
    ## $ subject    <chr> "TSE", "TSE", "TSE", "TSE", "TSE", "TSE", "TSE", "TSE", ...
    ## $ measure    <chr> "PC_GDP", "PC_GDP", "PC_GDP", "PC_GDP", "PC_GDP", "PC_GD...
    ## $ frequency  <chr> "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "...
    ## $ time       <dbl> 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 19...
    ## $ value      <dbl> 0.8393220, 0.6187854, 0.6293805, 0.5628670, 0.5373301, 0...
    ## $ flag_codes <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...

Filter for PSE values in % of gross farm receipts

``` r
support <- support %>% 
  filter(subject == "PSE", measure == "PC_GFARM") 

nrow(support)
```

    ## [1] 743

Data shrieked significantly

From the data we need only 3 columns: Location, year and value Lets drop
other columns

``` r
support <-  support %>% 
  select(location, year = time, pse = value)

head(support)
```

    ## # A tibble: 6 x 3
    ##   location  year   pse
    ##   <chr>    <dbl> <dbl>
    ## 1 AUS       1986 12.9 
    ## 2 AUS       1987  9.11
    ## 3 AUS       1988  9.33
    ## 4 AUS       1989  8.64
    ## 5 AUS       1990  9.36
    ## 6 AUS       1991  8.76

Seems correct Now we need to join PSE indicator to out main data set. To
do so, we need to unify country names across data sets. Seems that
contrycode package might really help.

``` r
guess_field(support$location)
```

    ##              code percent_of_unique_matched
    ## genc3c     genc3c                  92.59259
    ## iso3c       iso3c                  92.59259
    ## wb             wb                  92.59259
    ## wb_api3c wb_api3c                  92.59259

Seems that OECD uses one of beforewritten country codes lets convert it
to the regular English country names

``` r
support$country <- countrycode(sourcevar = support$location,
                    origin = "genc3c",
                    destination = "country.name",
                    warn = TRUE,
                    nomatch = NULL)
```

lets check the results

``` r
support %>% 
  distinct(location, country) %>% 
  head()
```

    ## # A tibble: 6 x 2
    ##   location country    
    ##   <chr>    <chr>      
    ## 1 AUS      Australia  
    ## 2 CAN      Canada     
    ## 3 ISL      Iceland    
    ## 4 JPN      Japan      
    ## 5 KOR      South Korea
    ## 6 MEX      Mexico

Beautiful

Drop columns we don’t need

``` r
support <- support %>% 
  select(country, year, pse)
```

Since OECD data not provide PSE data related to the individual EU
countries we will have to use EU28 aggregate PSE information

``` r
EU28 <- countrycode(
        sourcevar = c(
        "Austria",  "Italy",
        "Belgium",  "Latvia",
        "Bulgaria", "Lithuania",
        "Croatia",  "Luxembourg",
        "Cyprus",     "Malta",
        "Czech Republic",   "Netherlands",
        "Denmark",  "Poland",
        "Estonia",  "Portugal",
        "Finland",  "Romania",
        "France",     "Slovakia",
        "Germany",  "Slovenia",
        "Greece",     "Spain",
        "Hungary",  "Sweden",
        "Ireland", "United Kingdom"
                      ),
            origin = "country.name",
            destination = "country.name",
            warn = TRUE,
            nomatch = NULL)
```

Lets estimate average PSE related to each MPI observation

Fist of all let’s make a column which contain all the years during which
each MPI was estimated

``` r
country_year <- clean %>%
  select(country_st, obs_start, obs_stop) %>%
  # change names of EU countries to "EU28" to be able to join with

  # create unique identification of each MPI
  tibble::rowid_to_column("id") %>%   
  # transform start and end date to one column 
  gather(key = "start_stop", value = "year", obs_start:obs_stop) %>% 
  arrange(id) %>% 
  # convert this new column as date
  mutate(year = as.Date(as.character(year), format = "%Y")) %>% 
  group_by(id) %>% 
  # fill sequence of of years
  complete(year = seq(min(year), max(year), by ="year")) %>% 
  ungroup() %>% 
  #drop start_stop columns since we don't need it
  select(id, country = country_st, year) %>% 
  # fill empty values
  fill(country, .direction = "down") %>%
  # convert datetype to keep just year information
  # and drop moth and day
  mutate(year = year(year))

country_year %>% head()
```

    ## # A tibble: 6 x 3
    ##      id country  year
    ##   <int> <chr>   <dbl>
    ## 1     1 Germany  1995
    ## 2     1 Germany  1996
    ## 3     1 Germany  1997
    ## 4     1 Germany  1998
    ## 5     1 Germany  1999
    ## 6     1 Germany  2000

Now we can easily join country\_year with PSE data to calculate the
average PSE per MPI estimate

``` r
mean_pse <- country_year %>% 
  # change EU country names to match OECD aggregation 
  # OECD data
  mutate(country = case_when(
                        country %in% EU28 ~ "EU28",
                        TRUE ~ country)) %>% 
  # join information related to PSE index
  left_join(support, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average PSE index attributed to each MPI observation
  group_by(id) %>% 
  summarise(mean_pse = mean(pse, na.rm=TRUE)) %>% 
  ungroup()
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

``` r
head(mean_pse)
```

    ## # A tibble: 6 x 2
    ##      id mean_pse
    ##   <int>    <dbl>
    ## 1     1     34.7
    ## 2     2     34.7
    ## 3     3     34.7
    ## 4     4     34.7
    ## 5     5    NaN  
    ## 6     6    NaN

``` r
mean(is.na(mean_pse$mean_pse))
```

    ## [1] 0.04545455

Unfortunately we 4.5% of observations is missing Probably this is
because many articles use too old data for the MPI estimation While PSE
index data is available starting from 1986,

``` r
clean <- clean %>% 
  # create an id for each of MPI observation to join it to PSE data
  tibble::rowid_to_column("id") %>% 
  inner_join(mean_pse, by = "id", copy = TRUE)
```

Now we have PSE information in out data set

Plot the resulting variable

``` r
ggplot(clean, aes(mean_pse, mp_index, color=approach)) +
  geom_point(alpha = 0.7, size = 5, shape = 1) +
  theme_minimal()
```

    ## Warning: Removed 10 rows containing missing values (geom_point).

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-52-1.png)<!-- -->

Check observations with negative PSE

``` r
clean %>% 
  filter(!is.na(mean_pse), mean_pse < 0) %>% 
  select(country_st, id, mean_pse) %>% 
  mutate(log_mean_pse = log(mean_pse))
```

    ## Warning in log(mean_pse): NaNs produced

    ## # A tibble: 11 x 4
    ##    country_st    id mean_pse log_mean_pse
    ##    <chr>      <int>    <dbl>        <dbl>
    ##  1 Ukraine      111   -1.02           NaN
    ##  2 India        113   -0.208          NaN
    ##  3 Ukraine      118   -1.02           NaN
    ##  4 Ukraine      119   -1.02           NaN
    ##  5 Ukraine      120   -1.02           NaN
    ##  6 Ukraine      121   -1.02           NaN
    ##  7 Ukraine      122   -1.02           NaN
    ##  8 Ukraine      123   -1.02           NaN
    ##  9 Ukraine      124   -1.02           NaN
    ## 10 Ukraine      125   -1.02           NaN
    ## 11 Ukraine      126   -1.02           NaN

Ukraine ¯\_(ツ)\_/¯

### Adding Producer Protection (PP) index to our data

The algorithm will be the similar as in case of PSE so I will not
comment it

Data source is the same
[sorce](https://data.oecd.org/agrpolicy/producer-protection.htm#indicator-chart)

``` r
PP <- read_csv( here("data", "raw data",
                     "Producer Protection (PP) OECD.csv") )
```

    ## Parsed with column specification:
    ## cols(
    ##   LOCATION = col_character(),
    ##   INDICATOR = col_character(),
    ##   SUBJECT = col_character(),
    ##   MEASURE = col_character(),
    ##   FREQUENCY = col_character(),
    ##   TIME = col_double(),
    ##   Value = col_double(),
    ##   `Flag Codes` = col_logical()
    ## )

``` r
PP <- PP %>% 
  clean_names()

glimpse(PP)
```

    ## Rows: 743
    ## Columns: 8
    ## $ location   <chr> "AUS", "AUS", "AUS", "AUS", "AUS", "AUS", "AUS", "AUS", ...
    ## $ indicator  <chr> "AGRPNPC", "AGRPNPC", "AGRPNPC", "AGRPNPC", "AGRPNPC", "...
    ## $ subject    <chr> "TOT", "TOT", "TOT", "TOT", "TOT", "TOT", "TOT", "TOT", ...
    ## $ measure    <chr> "RT", "RT", "RT", "RT", "RT", "RT", "RT", "RT", "RT", "R...
    ## $ frequency  <chr> "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "...
    ## $ time       <dbl> 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 19...
    ## $ value      <dbl> 1.111986, 1.067421, 1.077049, 1.071517, 1.084206, 1.0710...
    ## $ flag_codes <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...

``` r
PP <-  PP %>% 
  select(location, year = time, PP = value)

head(PP)
```

    ## # A tibble: 6 x 3
    ##   location  year    PP
    ##   <chr>    <dbl> <dbl>
    ## 1 AUS       1986  1.11
    ## 2 AUS       1987  1.07
    ## 3 AUS       1988  1.08
    ## 4 AUS       1989  1.07
    ## 5 AUS       1990  1.08
    ## 6 AUS       1991  1.07

``` r
PP$country <- countrycode(sourcevar = PP$location,
                    origin = "genc3c",
                    destination = "country.name",
                    warn = TRUE,
                    nomatch = NULL)

PP %>% 
  distinct(location, country) %>% 
  head()
```

    ## # A tibble: 6 x 2
    ##   location country    
    ##   <chr>    <chr>      
    ## 1 AUS      Australia  
    ## 2 CAN      Canada     
    ## 3 ISL      Iceland    
    ## 4 JPN      Japan      
    ## 5 KOR      South Korea
    ## 6 MEX      Mexico

``` r
PP <- PP %>% 
  select(country, year, PP)
```

``` r
mean_pp <- country_year %>%
  mutate(country = case_when(
                      country %in% EU28 ~ "EU28",
                      TRUE ~ country)) %>% 
  left_join(PP, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average PSE index attributed to each MPI observation
  group_by(id) %>% 
  summarise(mean_pp = mean(PP, na.rm=TRUE)) %>% 
  ungroup()
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

``` r
head(mean_pp)
```

    ## # A tibble: 6 x 2
    ##      id mean_pp
    ##   <int>   <dbl>
    ## 1     1    1.35
    ## 2     2    1.35
    ## 3     3    1.35
    ## 4     4    1.35
    ## 5     5  NaN   
    ## 6     6  NaN

``` r
mean(is.na(mean_pp))
```

    ## [1] 0.02272727

Luckily there is only 2.2% of missing values in mean PP indicator

``` r
clean <- clean %>% 
  inner_join(mean_pp, by = "id", copy = TRUE)
```

Maybe I should not use those 4 outliers in the analysis

## Small Farms Share

Lets add variable witch contains information related to the market share
of small farmers

The EU data is taken from the Eurostat website The code of the data set
is “ef\_kvftaa” In the our analysis we define small farmers as ones who
have less than 20 ha of land. Small farmers variable will contain the
share of small farmers output (in EUR) in total agricultural output.

MAYBE IT IS BETTER TO USE INDUSTRY DATA, NOT COUNTRY AGGREGATE

``` r
farms <- read_csv(here("data", "raw data", "Farm structure EU",
                       "ef_kvftaa_1_Data.csv"),
                  na = c("", "NA", ":") # NA is coded as ":"
                  )
```

    ## Parsed with column specification:
    ## cols(
    ##   INDIC_EF = col_character(),
    ##   AGRAREA = col_character(),
    ##   GEO = col_character(),
    ##   FARMTYPE = col_character(),
    ##   TIME = col_double(),
    ##   Value = col_number()
    ## )

drop columns we don’t need and rename ones we need

``` r
farms <- farms %>% 
  clean_names() %>% 
  select(country = geo, agrarea, year = time, output_eur = value)

glimpse(farms)
```

    ## Rows: 1,280
    ## Columns: 4
    ## $ country    <chr> "Belgium", "Belgium", "Belgium", "Belgium", "Bulgaria", ...
    ## $ agrarea    <chr> "Total", "Total", "Total", "Total", "Total", "Total", "T...
    ## $ year       <dbl> 2005, 2007, 2010, 2013, 2005, 2007, 2010, 2013, 2005, 20...
    ## $ output_eur <dbl> 6785150200, 6638349610, 7247768310, 8406674190, 23212809...

Standardize country names

``` r
guess_field(farms$country) %>% head()
```

    ##                              code percent_of_unique_matched
    ## country.name.en   country.name.en                    96.875
    ## cldr.name.en         cldr.name.en                    96.875
    ## cldr.name.en_001 cldr.name.en_001                    96.875
    ## cldr.name.en_au   cldr.name.en_au                    96.875
    ## cldr.name.fil       cldr.name.fil                    96.875
    ## cow.name                 cow.name                    93.750

``` r
farms$country <- countrycode(sourcevar = farms$country,
                    origin = "country.name.en",
                    destination = "country.name",
                    warn = TRUE,
                    nomatch = NULL)
```

check the levels of farm size aggregation

``` r
farms %>% 
  pivot_wider(names_from = agrarea, 
              values_from = output_eur) %>% 
  clean_names() %>% 
  colnames()
```

    ##  [1] "country"            "year"               "total"             
    ##  [4] "zero_ha"            "less_than_2_ha"     "from_2_to_4_9_ha"  
    ##  [7] "from_5_to_9_9_ha"   "from_10_to_19_9_ha" "from_20_to_29_9_ha"
    ## [10] "from_30_to_49_9_ha" "from_50_to_99_9_ha" "x100_ha_or_over"

create column with share of small farms Also we will assume that in the
2000 year small farm market share == 2005 and 2018 == 2013 to alleviate
issue of sample size reduction.

``` r
sfarm_share <- farms %>%
  pivot_wider(names_from = agrarea, 
              values_from = output_eur) %>% 
  group_by(country, year) %>%
  clean_names() %>% 
  mutate(sfarms_total = zero_ha + less_than_2_ha + from_2_to_4_9_ha +
                        from_5_to_9_9_ha + from_10_to_19_9_ha,
         sfarms_share = sfarms_total / total * 100) %>% 
  ungroup() %>% 
  # add data for 2000 and 2018
  select(country, year, sfarms_share) %>% 
  pivot_wider(names_from = year, values_from = sfarms_share) %>%
  mutate(`2000` = `2005`, `2018` = `2013`, `1995` = `2005`) %>% 
  pivot_longer(cols = -country,
               names_to = "year",
               values_to = "sfarm_share") %>% 
  mutate(year = as.numeric(year))

head(sfarm_share)
```

    ## # A tibble: 6 x 3
    ##   country  year sfarm_share
    ##   <chr>   <dbl>       <dbl>
    ## 1 Belgium  2005        37.0
    ## 2 Belgium  2007        35.2
    ## 3 Belgium  2010        32.2
    ## 4 Belgium  2013        27.5
    ## 5 Belgium  2000        37.0
    ## 6 Belgium  2018        27.5

join sfarm\_share with main data set

``` r
sfarm <-country_year %>%
  left_join(sfarm_share, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average PSE index attributed to each MPI observation
  group_by(id) %>% 
  summarise(sfarm_share = mean(sfarm_share, na.rm=TRUE)) %>% 
  ungroup()
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

``` r
head(sfarm_share)
```

    ## # A tibble: 6 x 3
    ##   country  year sfarm_share
    ##   <chr>   <dbl>       <dbl>
    ## 1 Belgium  2005        37.0
    ## 2 Belgium  2007        35.2
    ## 3 Belgium  2010        32.2
    ## 4 Belgium  2013        27.5
    ## 5 Belgium  2000        37.0
    ## 6 Belgium  2018        27.5

``` r
sum(is.na(sfarm))
```

    ## [1] 179

A lot of missing observations

``` r
clean <- clean %>% 
  inner_join(sfarm, by = "id", copy = TRUE)
```

``` r
ggplot(clean, aes(sfarm_share, mp_index, color=approach)) +
  geom_point(alpha = 0.4, size = 5) +
  theme_minimal()
```

    ## Warning: Removed 179 rows containing missing values (geom_point).

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-72-1.png)<!-- -->

## Add data from Doing Buisiness

``` r
db <- readxl::read_excel(path = here("data", "raw data", "Doing Business.xlsx"),
                         sheet = 1, skip = 3)
```

    ## New names:
    ## * `Procedures (number)` -> `Procedures (number)...29`
    ## * `Score-Procedures (number)` -> `Score-Procedures (number)...30`
    ## * `Time (days)` -> `Time (days)...31`
    ## * `Score-Time (days)` -> `Score-Time (days)...32`
    ## * `Procedures (number)` -> `Procedures (number)...46`
    ## * ...

``` r
dim(db)
```

    ## [1] 3606  199

select only columns we need

``` r
db <- db %>%
  clean_names() %>% 
  select(country_code, year = db_year, score_starting_a_business)
```

``` r
head(db, 10)
```

    ## # A tibble: 10 x 3
    ##    country_code  year score_starting_a_business
    ##    <chr>        <dbl>                     <dbl>
    ##  1 AFG           2020                      92.0
    ##  2 AFG           2019                      92.0
    ##  3 AFG           2018                      82.6
    ##  4 AFG           2017                      90.4
    ##  5 AFG           2016                      90.5
    ##  6 AFG           2015                      91.0
    ##  7 AFG           2014                      91.6
    ##  8 AFG           2013                      88.3
    ##  9 AFG           2012                      87.9
    ## 10 AFG           2011                      87.8

To decrease the number of dropped observations we assume that there is
significant difference between 2004 and 2000 in Start Business index

``` r
db <- db %>% 
  pivot_wider(names_from = year, 
              values_from = score_starting_a_business) %>% 
  mutate(`2000` = `2004`) %>% 
  pivot_longer(cols = -country_code,
               names_to = "year",
               values_to = "score_starting_a_business") %>% 
  mutate(year = as.numeric(year))
```

Adjust country names

``` r
guess_field(db$country_code)
```

    ##              code percent_of_unique_matched
    ## genc3c     genc3c                  87.32394
    ## iso3c       iso3c                  87.32394
    ## wb             wb                  87.32394
    ## wb_api3c wb_api3c                  87.32394

``` r
db$country <- countrycode(sourcevar = db$country_code,
                    origin = "genc3c",
                    destination = "country.name",
                    warn = TRUE,
                    nomatch = NULL)
```

``` r
db %>% 
  select(country_code, country) %>% 
  distinct() %>% 
  head(10)
```

    ## # A tibble: 10 x 2
    ##    country_code country          
    ##    <chr>        <chr>            
    ##  1 AFG          Afghanistan      
    ##  2 ALB          Albania          
    ##  3 DZA          Algeria          
    ##  4 AGO          Angola           
    ##  5 ATG          Antigua & Barbuda
    ##  6 ARG          Argentina        
    ##  7 ARM          Armenia          
    ##  8 AUS          Australia        
    ##  9 AUT          Austria          
    ## 10 AZE          Azerbaijan

``` r
db_join <- country_year %>%
  left_join(db, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average DB index attributed to each MPI observation
  group_by(id) %>% 
  summarise(start_business = mean(score_starting_a_business, na.rm=TRUE)) %>% 
  ungroup()
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

``` r
head(db_join, 10)
```

    ## # A tibble: 10 x 2
    ##       id start_business
    ##    <int>          <dbl>
    ##  1     1           73.2
    ##  2     2           73.2
    ##  3     3           73.2
    ##  4     4           73.2
    ##  5     5          NaN  
    ##  6     6          NaN  
    ##  7     7          NaN  
    ##  8     8          NaN  
    ##  9     9           73.2
    ## 10    10           66.7

``` r
sum(is.na(db_join$start_business))
```

    ## [1] 88

``` r
clean <- clean %>% 
  inner_join(db_join, by = "id", copy = TRUE)
```

### Make an object only with columns we need for the further analysis

``` r
drop_col <- c("id", "year", "authors", "title", "country", "industry", "period",
              "obs_start", "obs_stop", "n_obs_per_year", "industry_ctg",
              "country_st", "obs_years")

analysis <- clean %>%
  # select all except of drop_col
  select(-all_of(drop_col)) %>% 
  rename(app = approach,
         freq = data_freq,
         type = mp_type,
         index = mp_index,
         ind = industry_fct,
         count = country_fct,
         obs_n = n_of_obs) %>%
  # create dummies from category columns
  fastDummies::dummy_columns(select_columns = c("freq", "type", "ind",
                                                "count", "app"),
                remove_selected_columns = TRUE)
```

``` r
saveRDS(clean, file = here("data", "clean data", "Full_data.Rds"))
saveRDS(analysis, file = here("data", "clean data", "Analysis_data.Rds"))
```
