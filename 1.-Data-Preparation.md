Data Preparation
================
Mykola Dereva
24/06/2020

## Importing data

``` r
data <- read_csv(here("data", "raw data",
                      "Market Power Articles - Final2.csv"))
```

    ## Warning: Missing column names filled in: 'X16' [16], 'X17' [17]

    ## 
    ## -- Column specification --------------------------------------------------------
    ## cols(
    ##   Year = col_double(),
    ##   Authors = col_character(),
    ##   Title = col_character(),
    ##   Country = col_character(),
    ##   `Industry/Market` = col_character(),
    ##   Approach = col_character(),
    ##   Period = col_character(),
    ##   `Data Frequency` = col_character(),
    ##   `Type of MP` = col_character(),
    ##   `Aggregation Level` = col_character(),
    ##   `Supply Chain Level` = col_character(),
    ##   `Lerner Index` = col_character(),
    ##   `Conjectural Variation` = col_character(),
    ##   `Web Of Science` = col_character(),
    ##   `Market Power Index` = col_double(),
    ##   X16 = col_character(),
    ##   X17 = col_character()
    ## )

Lets clean the table a bit

``` r
clean <- data %>%
  select(-starts_with("X1")) %>% # Drop technical columns (X11, X12)
  fill(Year, Authors, Title, Country, .direction = "down") %>% # Fill empty cells
  clean_names() %>% 
  rename(mp_type = type_of_mp,            # Shorten colnames
         mp_index = market_power_index,
         industry = industry_market,
         data_freq = data_frequency,
         agg_level = aggregation_level,
         SC_level = supply_chain_level,
         LI = lerner_index,
         CV = conjectural_variation,
         WOS = web_of_science) %>% 
  mutate(mp_index = as.numeric(mp_index))
  

glimpse(clean)
```

    ## Rows: 587
    ## Columns: 15
    ## $ year      <dbl> 2008, 2008, 2008, 2008, 1982, 1982, 1982, 1982, 1982, 198...
    ## $ authors   <chr> "Anders, S. M", "Anders, S. M", "Anders, S. M", "Anders, ...
    ## $ title     <chr> "Imperfect Competition in German Food Retailing: Evidence...
    ## $ country   <chr> "Germany", "Germany", "Germany", "Germany", "USA", "USA",...
    ## $ industry  <chr> "Beef", "Beef", "Pork", "Pork", "Textile", "Tobacco", "Ru...
    ## $ approach  <chr> "GIM", "GIM", "GIM", "GIM", "PTA", "PTA", "PTA", "PTA", "...
    ## $ period    <chr> "1995-2000", "1995-2000", "1995-2000", "1995-2000", "1947...
    ## $ data_freq <chr> "Monthly", "Monthly", "Monthly", "Monthly", "Yearly", "Ye...
    ## $ mp_type   <chr> "Oligopsony", "Oligopoly", "Oligopsony", "Oligopoly", "Ol...
    ## $ agg_level <chr> "Country", "Country", "Country", "Country", "Country", "C...
    ## $ SC_level  <chr> "Retail", "Retail", "Retail", "Retail", "Processor", "Pro...
    ## $ LI        <chr> "No", "No", "No", "No", "No", "No", "No", "Yes", "Yes", "...
    ## $ CV        <chr> "No", "No", "No", "No", "No", "No", "No", "No", "No", "No...
    ## $ WOS       <chr> "Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "...
    ## $ mp_index  <dbl> 0.17600, 0.08900, 0.01100, 0.00300, 0.03684, 0.40190, 0.0...

Check for the NA

``` r
clean %>% 
  summarise_all(~ sum(is.na(.x))) %>% 
  gather("column", "n.na")
```

    ## # A tibble: 15 x 2
    ##    column     n.na
    ##    <chr>     <int>
    ##  1 year          0
    ##  2 authors       0
    ##  3 title         0
    ##  4 country       0
    ##  5 industry      0
    ##  6 approach      0
    ##  7 period        0
    ##  8 data_freq     0
    ##  9 mp_type       0
    ## 10 agg_level     0
    ## 11 SC_level      0
    ## 12 LI            0
    ## 13 CV            0
    ## 14 WOS           0
    ## 15 mp_index      0

### Clean and convert approach column

``` r
clean %>%
  group_by(approach) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## # A tibble: 4 x 3
    ##   approach     n  prop
    ##   <chr>    <int> <dbl>
    ## 1 SFA        269 45.8 
    ## 2 PTA        173 29.5 
    ## 3 GIM        134 22.8 
    ## 4 Other       11  1.87

To Factor:

``` r
clean <- clean %>%
  mutate(approach = as_factor(approach))
```

``` r
clean %>%
  ggplot(aes(approach, mp_index)) +
  geom_jitter(width = 0.3, size = 2, alpha = 0.5)
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

Seems correct

#### Converting mp\_type to categorical dtype

``` r
clean %>%
  group_by(mp_type) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## # A tibble: 2 x 3
    ##   mp_type        n  prop
    ##   <chr>      <int> <dbl>
    ## 1 Oligopoly    363  61.8
    ## 2 Oligopsony   224  38.2

``` r
clean <- clean %>%
  mutate(mp_type = as_factor(mp_type))
```

``` r
clean %>%
  ggplot(aes(approach, mp_index, color = mp_type)) +
  geom_jitter(width = 0.3, size = 2, alpha = 0.5)
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->
Seems ok

### Clean and convert data\_freq colum

``` r
clean %>%
  group_by(data_freq) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## # A tibble: 5 x 3
    ##   data_freq     n  prop
    ##   <chr>     <int> <dbl>
    ## 1 Yearly      524 89.3 
    ## 2 Monthly      50  8.52
    ## 3 Quarterly    10  1.7 
    ## 4 Weekly        2  0.34
    ## 5 Daily         1  0.17

Seems like weekly and daily are redundant

``` r
clean <- clean %>%
  mutate(data_freq = as.factor(data_freq)) %>% 
  mutate(data_freq_fct = fct_lump_min(data_freq, min = 10))
```

``` r
clean %>%
  ggplot(aes(x = fct_reorder(data_freq_fct, mp_index, length),
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.3, height = 0, alpha = 0.3, size = 2) +
  labs(x = "Industry", y = "Market Power Index") +
  theme_minimal()
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-13-1.png)<!-- -->

Seems fine

Also I’ll create new column with number of observations per year

``` r
clean <- clean %>%
  mutate(n_obs_per_year = case_when(data_freq == "Yearly" ~ 1,
                                    data_freq == "Monthly" ~ 12,
                                    data_freq == "Quarterly" ~ 4,
                                    data_freq == "Daily" ~ 365,
                                    data_freq == "Weekly" ~ 52 ) )
```

``` r
clean %>%
  group_by(n_obs_per_year) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## # A tibble: 5 x 3
    ##   n_obs_per_year     n  prop
    ##            <dbl> <int> <dbl>
    ## 1              1   524 89.3 
    ## 2             12    50  8.52
    ## 3              4    10  1.7 
    ## 4             52     2  0.34
    ## 5            365     1  0.17

seems correct Most MP observations have Yearly frequency

### Clean and transform industry column

``` r
clean %>%
  group_by(industry) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## # A tibble: 204 x 3
    ##    industry                  n  prop
    ##    <chr>                 <int> <dbl>
    ##  1 Dairy                    72 12.3 
    ##  2 Slaughtering             48  8.18
    ##  3 Fruits and vegetables    47  8.01
    ##  4 Milling                  47  8.01
    ##  5 Milk                     14  2.39
    ##  6 Coffee                   12  2.04
    ##  7 Barley                   10  1.7 
    ##  8 Canola                   10  1.7 
    ##  9 Wheat                    10  1.7 
    ## 10 Oats                      8  1.36
    ## # ... with 194 more rows

With this one it will be a bit more complicated since there are plenty
of categories

I couldn’t quickly figure out how tho check the presence of every value
in vectors in the industry\_fct column for categorization.

So I will write custom function which returns TRUE if column contains
any value in the vector and FALSE in not.

``` r
# helper function. Returns TRUE if any value in vector match string
contains_value <- function(string, vector) {
  
  string <- str_to_lower(string) %>% str_trim(side = "both")
  
  matched <- FALSE
  
  for (word in vector) {
    
    if ( any(str_detect(string, word)) ) {
      matched <- TRUE
      break }
  }
  
  return(matched)
}
```

And another function which will classify industry into the given groups
Function that takes a industry string and classify it into given set of
groups function will check if any key-words is contained in the input
string

``` r
classify_industry <- function(non_classified_str) {
  
  # define classification vectors
  
  dairy <- c("milk", "dairy", "cheese", "butter")
  meat <- c("meat", "pork", "beef", "hog", "poultry", "livestock", "cattle",
            "paultry", "pig", "slaughtering")
  cereals <- c("wheat", "oat", "triticale", "grain", "cereal", "barley", "rice",
               "milling")
  oils <- c("canola", "oil", "fat", "margarine")
  beverages <- c("tea", "beer", "water", "brew", "coffee", "liquor", "drink",
                 "spirit", "wine", "brandy", "cocoa", "beverage")
  tobaco <- c("tobacco", "cigar")
  fruits <- c("fruit", "banana", "horticulture")
  paper <- c("pulp", "paper", "wood", "sawlog")
  bread_flour <- c("bread", "flour")
  sugar <- c("sugar")
  
  
  classified <- case_when(
        contains_value(non_classified_str, dairy) ~ "dairy",
        contains_value(non_classified_str, meat) ~ "meat",
        contains_value(non_classified_str, cereals) ~ "cereals",
        contains_value(non_classified_str, oils) ~ "oils",
        contains_value(non_classified_str, beverages) ~ "beverages",
        contains_value(non_classified_str, tobaco) ~ "tobacco",
        contains_value(non_classified_str, fruits) ~ "fruits",
        contains_value(non_classified_str, paper) ~ "paper",
        contains_value(non_classified_str, bread_flour) ~ "bread_or_flour",
        contains_value(non_classified_str, sugar) ~ "sugar",
        TRUE ~ "other"
                          )
  return(classified)
}
```

Check the function results

``` r
clean %>% 
  mutate(classified = map_chr(industry, classify_industry)) %>% 
  select(industry, classified)
```

    ## # A tibble: 587 x 2
    ##    industry classified
    ##    <chr>    <chr>     
    ##  1 Beef     meat      
    ##  2 Beef     meat      
    ##  3 Pork     meat      
    ##  4 Pork     meat      
    ##  5 Textile  other     
    ##  6 Tobacco  tobacco   
    ##  7 Rubber   other     
    ##  8 Textile  other     
    ##  9 Tobacco  tobacco   
    ## 10 Rubber   other     
    ## # ... with 577 more rows

``` r
clean <- clean %>% 
  mutate(industry_fct = map_chr(industry, classify_industry)) %>%
  mutate(industry_fct = as_factor(industry_fct))
```

Seems that the values were changed correctly

Lets aggregate the data in column

``` r
clean %>%
  group_by(industry_fct) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## # A tibble: 11 x 3
    ##    industry_fct       n  prop
    ##    <fct>          <int> <dbl>
    ##  1 cereals          115 19.6 
    ##  2 dairy            112 19.1 
    ##  3 meat              94 16.0 
    ##  4 other             84 14.3 
    ##  5 fruits            59 10.0 
    ##  6 beverages         52  8.86
    ##  7 oils              32  5.45
    ##  8 paper             11  1.87
    ##  9 bread_or_flour    11  1.87
    ## 10 tobacco           10  1.7 
    ## 11 sugar              7  1.19

``` r
clean %>%
  ggplot(aes(x = fct_reorder(industry_fct, mp_index, length),
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.25, height = 0, alpha = 0.6, size = 2) +
  labs(x = "Industry", y = "Market Power Index") +
  theme_minimal()
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-22-1.png)<!-- -->

Seems that there is an outlier in meat

### Clean country column

``` r
clean %>%
  group_by(country) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  ungroup() %>%
  arrange(-n) %>% 
  head(10)
```

    ## # A tibble: 10 x 3
    ##    country         n  prop
    ##    <chr>       <int> <dbl>
    ##  1 USA           207 35.3 
    ##  2 Australia      81 13.8 
    ##  3 Spain          28  4.77
    ##  4 Netherlands    20  3.41
    ##  5 Germany        17  2.9 
    ##  6 France         14  2.39
    ##  7 Italy          14  2.39
    ##  8 Hungary        12  2.04
    ##  9 Poland         10  1.7 
    ## 10 Portugal       10  1.7

Seems that countries can be categorized into 3 main groups USA, EU and
Australia I am not sure about Australia since most of the observations
are from single article.

I will use country names a lot with different data sets, so it is
important to make sure that I countries written in the same way
everywhere.

I will create small function which will standardize country names with
the help of countrycode package

``` r
standatise_country_name <- function(old_country_name, 
                                    initial_code_scheme = "country.name") {
  
  new_country_name <- countrycode(sourcevar = old_country_name,
                        origin = initial_code_scheme,
                        destination = "country.name",
                        warn = TRUE,
                        nomatch = NULL)
  }
```

check function

``` r
clean %>% 
  mutate(country_st = standatise_country_name(country)) %>% 
  distinct(country, country_st)
```

    ## # A tibble: 42 x 2
    ##    country     country_st   
    ##    <chr>       <chr>        
    ##  1 Germany     Germany      
    ##  2 USA         United States
    ##  3 Hungary     Hungary      
    ##  4 Sweden      Sweden       
    ##  5 Netherlands Netherlands  
    ##  6 World       World        
    ##  7 France      France       
    ##  8 Australia   Australia    
    ##  9 Canada      Canada       
    ## 10 Haiti       Haiti        
    ## # ... with 32 more rows

seems that it works well

``` r
clean <- clean %>% 
  mutate(country = standatise_country_name(country))
```

create country\_fct column

``` r
clean <- clean %>%
  mutate(country_fct = case_when(
              country %in% standardise_aes_names(
                #vector with European countries
                              c("Austria",  "Italy",
                                "Belgium",  "Latvia",
                                "Bulgaria", "Lithuania",
                                "Croatia",  "Luxembourg",
                                "Cyprus",     "Malta",
                                "Czech Republic",   "Netherlands",
                                "Denmark",  "Poland",
                                "Estonia",  "Portugal",
                                "Finland",  "Romania",
                                "France",     "Slovakia",
                                "Germany",  "Slovenia",
                                "Greece",     "Spain",
                                "Hungary",  "Sweden",
                                "Ireland",  "EU",
                                "Norway", "UK", "United Kingdom")
                  # all those countries will be set to europe
                                                    ) ~ "europe",
              country %in% c("United States", "Canada") ~ "n_america",
              TRUE ~ "other")
         ) %>%
  mutate(country_fct = as_factor(country_fct))
  
clean %>%
  group_by(country_fct) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## # A tibble: 3 x 3
    ##   country_fct     n  prop
    ##   <fct>       <int> <dbl>
    ## 1 europe        252  42.9
    ## 2 n_america     209  35.6
    ## 3 other         126  21.5

Seems fine.

``` r
clean %>%
  ggplot(aes(x = fct_reorder(country_fct, mp_index, median),
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.25, height = 0, alpha = 0.6, size = 2) +
  labs(x = "Geographical Position", y = "Market Power Index") +
  theme_minimal()
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-28-1.png)<!-- -->
\#\#\# Transform Aggregation Level

``` r
clean %>%
  group_by(agg_level) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## # A tibble: 4 x 3
    ##   agg_level     n  prop
    ##   <chr>     <int> <dbl>
    ## 1 Country     277 47.2 
    ## 2 Firm        214 36.5 
    ## 3 Region       93 15.8 
    ## 4 World         3  0.51

``` r
clean <- clean %>% 
  mutate(agg_level = as_factor(agg_level))
```

``` r
clean %>%
  ggplot(aes(x = agg_level,
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.25, height = 0, alpha = 0.6, size = 2) +
  labs(x = "Data aggregation level", y = "Market Power Index") +
  theme_minimal()
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-31-1.png)<!-- -->

### Supply Chain variable transform.

``` r
clean %>%
  group_by(SC_level) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## # A tibble: 2 x 3
    ##   SC_level      n  prop
    ##   <chr>     <int> <dbl>
    ## 1 Processor   554 94.4 
    ## 2 Retail       33  5.62

``` r
clean <- clean %>% 
  mutate(SC_level = as_factor(SC_level))
```

``` r
clean %>%
  ggplot(aes(x = SC_level,
             y = mp_index,
             color = mp_type)) + 
  geom_jitter(width = 0.25, height = 0, alpha = 0.4, size = 2) +
  labs(x = "Level of supply chain", y = "Market Power Index") +
  theme_minimal()
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-34-1.png)<!-- -->

### Lerner Index variable

``` r
clean %>%
  group_by(LI) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## # A tibble: 2 x 3
    ##   LI        n  prop
    ##   <chr> <int> <dbl>
    ## 1 No      373  63.5
    ## 2 Yes     214  36.5

``` r
clean <- clean %>%
  mutate(LI = if_else(LI == "Yes", true = 1, false = 0)) %>% 
  mutate(LI = as.integer(LI))
```

``` r
clean %>%
  ggplot(aes(x = LI,
             y = mp_index,
             color = mp_type)) + 
  geom_jitter(width = 0.25, height = 0, alpha = 0.4, size = 2) +
  labs(x = "Level of supply chain", y = "Market Power Index") +
  theme_minimal()
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-37-1.png)<!-- -->

### Conjectual Elasticity

``` r
clean %>%
  group_by(CV) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## # A tibble: 2 x 3
    ##   CV        n  prop
    ##   <chr> <int> <dbl>
    ## 1 No      578 98.5 
    ## 2 Yes       9  1.53

``` r
clean <- clean %>%
  mutate(CV = if_else(CV == "Yes", true = 1, false = 0)) %>% 
  mutate(CV = as.integer(CV))
```

### Web of Science dummy

``` r
clean %>%
  group_by(WOS) %>%
  summarise(n = n()) %>%
  mutate(prop = round(n / sum(n) * 100, 2)) %>%
  arrange(-n)
```

    ## # A tibble: 2 x 3
    ##   WOS       n  prop
    ##   <chr> <int> <dbl>
    ## 1 No      335  57.1
    ## 2 Yes     252  42.9

``` r
clean <- clean %>%
  mutate(WOS = if_else(WOS == "Yes", true = 1, false = 0)) %>% 
  mutate(WOS = as.integer(WOS))
```

### add column with number of observations in article

``` r
clean <- clean %>%
  separate(period, into = c("obs_start", "obs_stop"), 
           convert = TRUE, remove = FALSE) %>%
  mutate(obs_years = obs_stop - obs_start) %>%
  mutate(n_of_obs = n_obs_per_year * obs_years)
```

``` r
clean %>%
  select(obs_start, obs_stop, obs_years, n_obs_per_year, n_of_obs) %>%
  head(10)
```

    ## # A tibble: 10 x 5
    ##    obs_start obs_stop obs_years n_obs_per_year n_of_obs
    ##        <int>    <int>     <int>          <dbl>    <dbl>
    ##  1      1995     2000         5             12       60
    ##  2      1995     2000         5             12       60
    ##  3      1995     2000         5             12       60
    ##  4      1995     2000         5             12       60
    ##  5      1947     1971        24              1       24
    ##  6      1947     1971        24              1       24
    ##  7      1947     1971        24              1       24
    ##  8      1947     1971        24              1       24
    ##  9      1947     1971        24              1       24
    ## 10      1947     1971        24              1       24

Seems correct

``` r
clean %>%
  ggplot(aes(x = n_of_obs, y = mp_index, color = approach)) +
  geom_point(size = 3, alpha = 0.4) +
  scale_x_continuous(trans = "log10") +
  theme_minimal() +
  easy_move_legend(to = "bottom") +
    labs(x = "Number of observations (log10)",
         y = "Market Power Index") +
  easy_add_legend_title("Approach used:")
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-44-1.png)<!-- -->

### Create dummy variable after 2005

Hypothesis is that there might me a difference between in mp index
between older articles and newer ones. The decision to choose 2005 is a
bit arbitrary, but it divide data in 2 sets with approximately the same
number of observations.

``` r
clean <- clean %>%
  mutate(
          after_2005 = ifelse(
            year > 2005,
            yes = 1,
            no = 0) 
        ) 

clean %>%
  select(year, after_2005) %>%
  head(10)
```

    ## # A tibble: 10 x 2
    ##     year after_2005
    ##    <dbl>      <dbl>
    ##  1  2008          1
    ##  2  2008          1
    ##  3  2008          1
    ##  4  2008          1
    ##  5  1982          0
    ##  6  1982          0
    ##  7  1982          0
    ##  8  1982          0
    ##  9  1982          0
    ## 10  1982          0

Seems correct

``` r
clean %>%
  ggplot(aes(x = as_factor(after_2005),
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.25, height = 0, alpha = 0.6, size = 2) +
  labs(x = "After 2005", y = "Market Power Index") +
  theme_minimal()
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-46-1.png)<!-- -->

### Dummy for perishable goods

``` r
clean <- clean %>%
  mutate(
          perish = case_when(
                        map_lgl(industry, ~ contains_value(.x,
                        # if industry contains any of values below it is considered
                        # to be perishable and it will be set to 1 and 0 otherwise
                                      c("beef", "pork", "meat", "milk",
                                        "paultry", "cheese", "bakery",
                                        "poultry", "egg", "fish",
                                        "salmon", "hog", "dairy",
                                        "cake", "cattle", "livestock",
                                        "fruit", "slaughter"))) ~ 1,
                        
                                        TRUE ~ 0) )  %>%
  # set 0 to non perishable goods which mistakenly was misclassified
  mutate(perish = case_when(
                        map_lgl(industry, ~ contains_value(.x,
                        # this chunk fix the bug that cond. milk and rise were
                        # classified as perishables
                                        c("cond", "rice", "frozen",
                                          "dried", "canned"))) ~ 0,
                                        TRUE ~ perish) )
```

``` r
clean <- clean %>% 
  mutate(perish = as.integer(perish))
```

check results

``` r
clean %>% 
  select(industry, perish) %>% 
  filter(perish == 1)
```

    ## # A tibble: 260 x 2
    ##    industry     perish
    ##    <chr>         <int>
    ##  1 Beef              1
    ##  2 Beef              1
    ##  3 Pork              1
    ##  4 Pork              1
    ##  5 Beef Packing      1
    ##  6 Meat              1
    ##  7 Livestock         1
    ##  8 Meat              1
    ##  9 Livestock         1
    ## 10 Hogs              1
    ## # ... with 250 more rows

Seems correct

``` r
clean %>%
  ggplot(aes(x = as_factor(perish),
             y = mp_index,
             color = approach)) + 
  geom_jitter(width = 0.25, height = 0, alpha = 0.6, size = 2) +
  labs(x = "Perishable", y = "Market Power Index") +
  theme_minimal()
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-50-1.png)<!-- -->

## Load data related to Agricultural support (PSE) and Producer protection

### Adding Agr. support (PSE) to our data

data is taken from OECD website
[sorce](https://data.oecd.org/agrpolicy/agricultural-support.htm)

``` r
support <- read_csv( here("data", "raw data",
                          "Agricultural support (PSE) OESD .csv") )
```

    ## 
    ## -- Column specification --------------------------------------------------------
    ## cols(
    ##   LOCATION = col_character(),
    ##   INDICATOR = col_character(),
    ##   SUBJECT = col_character(),
    ##   MEASURE = col_character(),
    ##   FREQUENCY = col_character(),
    ##   TIME = col_double(),
    ##   Value = col_double(),
    ##   `Flag Codes` = col_logical()
    ## )

We decided to use PSE index in our analysis But first, rename colnames
to make them easier to use.

``` r
support <- support %>% 
  clean_names()

glimpse(support)
```

    ## Rows: 8,848
    ## Columns: 8
    ## $ location   <chr> "AUS", "AUS", "AUS", "AUS", "AUS", "AUS", "AUS", "AUS", ...
    ## $ indicator  <chr> "AGRSUPP", "AGRSUPP", "AGRSUPP", "AGRSUPP", "AGRSUPP", "...
    ## $ subject    <chr> "TSE", "TSE", "TSE", "TSE", "TSE", "TSE", "TSE", "TSE", ...
    ## $ measure    <chr> "PC_GDP", "PC_GDP", "PC_GDP", "PC_GDP", "PC_GDP", "PC_GD...
    ## $ frequency  <chr> "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "...
    ## $ time       <dbl> 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 19...
    ## $ value      <dbl> 0.8393220, 0.6187854, 0.6293805, 0.5628670, 0.5373301, 0...
    ## $ flag_codes <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...

Filter for PSE values in % of gross farm receipts

``` r
support <- support %>% 
  filter(subject == "PSE", measure == "PC_GFARM") 
```

From the data we need only 3 columns: Location, year and value Lets drop
other columns

``` r
support <-  support %>% 
  select(location, year = time, pse = value)

head(support)
```

    ## # A tibble: 6 x 3
    ##   location  year   pse
    ##   <chr>    <dbl> <dbl>
    ## 1 AUS       1986 12.9 
    ## 2 AUS       1987  9.11
    ## 3 AUS       1988  9.33
    ## 4 AUS       1989  8.64
    ## 5 AUS       1990  9.36
    ## 6 AUS       1991  8.76

Seems correct Now we need to join PSE indicator to out main data set. To
do so, we need to unify country names across data sets. Seems that
contrycode package might really help.

``` r
guess_field(support$location)
```

    ##              code percent_of_unique_matched
    ## genc3c     genc3c                  92.59259
    ## iso3c       iso3c                  92.59259
    ## wb             wb                  92.59259
    ## wb_api3c wb_api3c                  92.59259

Seems that OECD uses one of beforewritten country codes lets convert it
to the regular English country names

``` r
support$country <- map_chr(support$location,
                           ~ standatise_country_name(.x, "genc3c"))
```

lets check the results

``` r
support %>% 
  distinct(location, country) %>% 
  head()
```

    ## # A tibble: 6 x 2
    ##   location country    
    ##   <chr>    <chr>      
    ## 1 AUS      Australia  
    ## 2 CAN      Canada     
    ## 3 ISL      Iceland    
    ## 4 JPN      Japan      
    ## 5 KOR      South Korea
    ## 6 MEX      Mexico

Beautiful

Drop columns we don’t need

``` r
support <- support %>% 
  select(country, year, pse)
```

Since OECD data not provide PSE data related to the individual EU
countries we will have to use EU28 aggregate PSE information

``` r
EU28 <- standatise_country_name( c(
                      "Austria",    "Italy",
                      "Belgium",    "Latvia",
                      "Bulgaria",   "Lithuania",
                      "Croatia",    "Luxembourg",
                      "Cyprus",   "Malta",
                      "Czech Republic", "Netherlands",
                      "Denmark",    "Poland",
                      "Estonia",    "Portugal",
                      "Finland",    "Romania",
                      "France",   "Slovakia",
                      "Germany",    "Slovenia",
                      "Greece",   "Spain",
                      "Hungary",    "Sweden",
                      "Ireland", "United Kingdom"
                                    )
                                )
```

Lets estimate average PSE related to each MPI observation

Fist of all let’s make a column which contain all the years during which
each MPI was estimated

``` r
country_year <- clean %>%
  select(country, obs_start, obs_stop) %>%
  # create unique identification of each MPI
  tibble::rowid_to_column("id") %>%   
  # transform start and end date to one column 
  gather(key = "start_stop", value = "year", obs_start:obs_stop) %>% 
  arrange(id) %>% 
  # convert this new column as date
  mutate(year = as.Date(as.character(year), format = "%Y")) %>% 
  group_by(id) %>% 
  # fill sequence of of years
  complete(year = seq(min(year), max(year), by = "year")) %>% 
  ungroup() %>% 
  #drop start_stop columns since we don't need it
  select(id, country, year) %>% 
  # fill empty values
  fill(country, .direction = "down") %>%
  # convert datetype to keep just year information
  # and drop moth and day
  mutate(year = year(year))

country_year %>% head()
```

    ## # A tibble: 6 x 3
    ##      id country  year
    ##   <int> <chr>   <dbl>
    ## 1     1 Germany  1995
    ## 2     1 Germany  1996
    ## 3     1 Germany  1997
    ## 4     1 Germany  1998
    ## 5     1 Germany  1999
    ## 6     1 Germany  2000

Now we can easily join country\_year with PSE data to calculate the
average PSE per MPI estimate

``` r
mean_pse <- country_year %>% 
  # change EU country names to match OECD aggregation 
  # OECD data
  mutate(country = case_when(
                        country %in% EU28 ~ "EU28",
                        TRUE ~ country)) %>% 
  # join information related to PSE index
  left_join(support, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average PSE index attributed to each MPI observation
  group_by(id) %>% 
  summarise(mean_pse = mean(pse, na.rm = TRUE)) %>% 
  ungroup()

head(mean_pse)
```

    ## # A tibble: 6 x 2
    ##      id mean_pse
    ##   <int>    <dbl>
    ## 1     1     34.7
    ## 2     2     34.7
    ## 3     3     34.7
    ## 4     4     34.7
    ## 5     5    NaN  
    ## 6     6    NaN

``` r
mean(is.na(mean_pse$mean_pse))
```

    ## [1] 0.05621806

Unfortunately around 5% of observations is missing Probably this is
because many articles use too old data for the MPI estimation While PSE
index data is available starting from 1986, or no data for the some
counties

``` r
clean <- clean %>% 
  # create an id for each of MPI observation to join it to PSE data
  tibble::rowid_to_column("id") %>% 
  inner_join(mean_pse, by = "id", copy = TRUE)
```

Now we have PSE information in out data set

Plot the resulting variable

``` r
ggplot(clean, aes(mean_pse, mp_index, color = approach)) +
  geom_point(alpha = 0.7, size = 5, shape = 1) +
  theme_minimal()
```

    ## Warning: Removed 33 rows containing missing values (geom_point).

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-64-1.png)<!-- -->

Check observations with negative PSE

``` r
clean %>% 
  filter( mean_pse < 0) %>% 
  select(country, id, mean_pse) %>% 
  mutate(log_mean_pse = log(mean_pse))
```

    ## Warning in log(mean_pse): NaNs produced

    ## # A tibble: 12 x 4
    ##    country    id mean_pse log_mean_pse
    ##    <chr>   <int>    <dbl>        <dbl>
    ##  1 Ukraine   158   -1.02           NaN
    ##  2 India     166   -0.208          NaN
    ##  3 India     169   -0.208          NaN
    ##  4 Ukraine   172   -1.02           NaN
    ##  5 Ukraine   173   -1.02           NaN
    ##  6 Ukraine   174   -1.02           NaN
    ##  7 Ukraine   175   -1.02           NaN
    ##  8 Ukraine   176   -1.02           NaN
    ##  9 Ukraine   177   -1.02           NaN
    ## 10 Ukraine   178   -1.02           NaN
    ## 11 Ukraine   179   -1.02           NaN
    ## 12 Ukraine   180   -1.02           NaN

Ukraine ¯\_(ツ)\_/¯

Countries with high PSE

``` r
clean %>% 
  filter(mean_pse > 60) %>% 
  select(id, country, mean_pse)
```

    ## # A tibble: 5 x 3
    ##      id country     mean_pse
    ##   <int> <chr>          <dbl>
    ## 1   187 South Korea     68.2
    ## 2   193 Japan           61.6
    ## 3   194 Japan           61.6
    ## 4   212 Norway          70.1
    ## 5   213 Norway          70.1

We are going to use log transformation of PSE later. Log is not defined
for negative values, so to not lose any observations because of this
issue. I will add constant to PSE. It shouldn’t influence the estimation

``` r
clean <- clean %>% 
  mutate(mean_pse = mean_pse + 3)
```

### Adding Producer Protection (PP) index to our data

The algorithm will be the similar as in case of PSE so I will not
comment it

Data source is the same
[sorce](https://data.oecd.org/agrpolicy/producer-protection.htm#indicator-chart)

``` r
PP <- read_csv( here("data", "raw data",
                     "Producer Protection (PP) OECD.csv") )
```

    ## 
    ## -- Column specification --------------------------------------------------------
    ## cols(
    ##   LOCATION = col_character(),
    ##   INDICATOR = col_character(),
    ##   SUBJECT = col_character(),
    ##   MEASURE = col_character(),
    ##   FREQUENCY = col_character(),
    ##   TIME = col_double(),
    ##   Value = col_double(),
    ##   `Flag Codes` = col_logical()
    ## )

``` r
PP <- PP %>% 
  clean_names()

glimpse(PP)
```

    ## Rows: 743
    ## Columns: 8
    ## $ location   <chr> "AUS", "AUS", "AUS", "AUS", "AUS", "AUS", "AUS", "AUS", ...
    ## $ indicator  <chr> "AGRPNPC", "AGRPNPC", "AGRPNPC", "AGRPNPC", "AGRPNPC", "...
    ## $ subject    <chr> "TOT", "TOT", "TOT", "TOT", "TOT", "TOT", "TOT", "TOT", ...
    ## $ measure    <chr> "RT", "RT", "RT", "RT", "RT", "RT", "RT", "RT", "RT", "R...
    ## $ frequency  <chr> "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "...
    ## $ time       <dbl> 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 19...
    ## $ value      <dbl> 1.111986, 1.067421, 1.077049, 1.071517, 1.084206, 1.0710...
    ## $ flag_codes <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...

``` r
PP <-  PP %>% 
  select(location, year = time, PP = value)

head(PP)
```

    ## # A tibble: 6 x 3
    ##   location  year    PP
    ##   <chr>    <dbl> <dbl>
    ## 1 AUS       1986  1.11
    ## 2 AUS       1987  1.07
    ## 3 AUS       1988  1.08
    ## 4 AUS       1989  1.07
    ## 5 AUS       1990  1.08
    ## 6 AUS       1991  1.07

``` r
PP$country <- map_chr(PP$location, ~ standatise_country_name(.x, "genc3c"))

PP %>% 
  distinct(location, country) %>% 
  head()
```

    ## # A tibble: 6 x 2
    ##   location country    
    ##   <chr>    <chr>      
    ## 1 AUS      Australia  
    ## 2 CAN      Canada     
    ## 3 ISL      Iceland    
    ## 4 JPN      Japan      
    ## 5 KOR      South Korea
    ## 6 MEX      Mexico

``` r
PP <- PP %>% 
  select(country, year, PP)
```

``` r
mean_pp <- country_year %>%
  mutate(country = case_when(
                      country %in% EU28 ~ "EU28",
                      TRUE ~ country)) %>% 
  left_join(PP, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average PP index attributed to each MPI observation
  group_by(id) %>% 
  summarise(mean_pp = mean(PP, na.rm = TRUE)) %>% 
  ungroup()

head(mean_pp)
```

    ## # A tibble: 6 x 2
    ##      id mean_pp
    ##   <int>   <dbl>
    ## 1     1    1.35
    ## 2     2    1.35
    ## 3     3    1.35
    ## 4     4    1.35
    ## 5     5  NaN   
    ## 6     6  NaN

``` r
clean <- clean %>% 
  inner_join(mean_pp, by = "id", copy = TRUE)
```

``` r
ggplot(clean, aes(mean_pp, mp_index, color = approach)) +
  geom_point(alpha = 0.7, size = 5, shape = 1) +
  theme_minimal()
```

    ## Warning: Removed 33 rows containing missing values (geom_point).

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-75-1.png)<!-- -->

Maybe I should not use those 4 outliers in the analysis

## Small Farms Share

Lets add variable witch contains information related to the market share
of small farmers

The EU data is taken from the Eurostat website The code of the data set
is “ef\_kvftaa” In the our analysis we define small farmers as ones who
have less than 20 ha of land. Small farmers variable will contain the
share of small farmers output (in EUR) in total agricultural output.

MAYBE IT IS BETTER TO USE INDUSTRY DATA, NOT COUNTRY AGGREGATE

``` r
farms <- read_csv(here("data", "raw data", "Farm structure EU",
                       "ef_kvftaa_1_Data.csv"),
                  na = c("", "NA", ":") # NA is coded as ":"
                  )
```

    ## 
    ## -- Column specification --------------------------------------------------------
    ## cols(
    ##   INDIC_EF = col_character(),
    ##   AGRAREA = col_character(),
    ##   GEO = col_character(),
    ##   FARMTYPE = col_character(),
    ##   TIME = col_double(),
    ##   Value = col_number()
    ## )

drop columns we don’t need and rename ones we need

``` r
farms <- farms %>% 
  clean_names() %>% 
  select(country = geo, agrarea, year = time, output_eur = value)

glimpse(farms)
```

    ## Rows: 1,280
    ## Columns: 4
    ## $ country    <chr> "Belgium", "Belgium", "Belgium", "Belgium", "Bulgaria", ...
    ## $ agrarea    <chr> "Total", "Total", "Total", "Total", "Total", "Total", "T...
    ## $ year       <dbl> 2005, 2007, 2010, 2013, 2005, 2007, 2010, 2013, 2005, 20...
    ## $ output_eur <dbl> 6785150200, 6638349610, 7247768310, 8406674190, 23212809...

Standardize country names

``` r
guess_field(farms$country) %>% head(3)
```

    ##                              code percent_of_unique_matched
    ## country.name.en   country.name.en                    96.875
    ## cldr.name.en         cldr.name.en                    96.875
    ## cldr.name.en_001 cldr.name.en_001                    96.875

``` r
farms$country <- standatise_country_name(farms$country)
```

check the levels of farm size aggregation

``` r
farms %>% 
  pivot_wider(names_from = agrarea, 
              values_from = output_eur) %>% 
  clean_names() %>% 
  colnames()
```

    ##  [1] "country"            "year"               "total"             
    ##  [4] "zero_ha"            "less_than_2_ha"     "from_2_to_4_9_ha"  
    ##  [7] "from_5_to_9_9_ha"   "from_10_to_19_9_ha" "from_20_to_29_9_ha"
    ## [10] "from_30_to_49_9_ha" "from_50_to_99_9_ha" "x100_ha_or_over"

create column with share of small farms Also we will assume that in the
2000 year small farm market share == 2005 and 2018 == 2013 to alleviate
issue of sample size reduction.

``` r
sfarm_share <- farms %>%
  pivot_wider(names_from = agrarea, 
              values_from = output_eur) %>% 
  group_by(country, year) %>%
  clean_names() %>% 
  mutate(sfarms = sum(zero_ha, less_than_2_ha, from_2_to_4_9_ha,
                        from_5_to_9_9_ha, from_10_to_19_9_ha, na.rm = TRUE),
         sfarms_share = sfarms / total * 100) %>% 
  ungroup() %>% 
  # add data for 2000 and 2018
  select(country, year, sfarms_share) %>% 
  pivot_wider(names_from = year, values_from = sfarms_share) %>%
  mutate(`2000` = `2005`, `2018` = `2013`, `1995` = `2005`) %>% 
  pivot_longer(cols = -country,
               names_to = "year",
               values_to = "sfarm_share") %>% 
  mutate(year = as.numeric(year))

head(sfarm_share)
```

    ## # A tibble: 6 x 3
    ##   country  year sfarm_share
    ##   <chr>   <dbl>       <dbl>
    ## 1 Belgium  2005        37.0
    ## 2 Belgium  2007        35.2
    ## 3 Belgium  2010        32.2
    ## 4 Belgium  2013        27.5
    ## 5 Belgium  2000        37.0
    ## 6 Belgium  2018        27.5

join sfarm\_share with main data set

``` r
sfarm <- country_year %>%
  left_join(sfarm_share, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average PSE index attributed to each MPI observation
  group_by(id) %>% 
  summarise(sfarm_share = mean(sfarm_share, na.rm = TRUE)) %>% 
  ungroup()

head(sfarm)
```

    ## # A tibble: 6 x 2
    ##      id sfarm_share
    ##   <int>       <dbl>
    ## 1     1        19.7
    ## 2     2        19.7
    ## 3     3        19.7
    ## 4     4        19.7
    ## 5     5       NaN  
    ## 6     6       NaN

``` r
sum(is.na(sfarm))
```

    ## [1] 356

A lot of missing observations :(

``` r
clean <- clean %>% 
  inner_join(sfarm, by = "id", copy = TRUE)
```

``` r
ggplot(clean, aes(sfarm_share, mp_index, color = approach)) +
  geom_point(alpha = 0.4, size = 5) +
  theme_minimal()
```

    ## Warning: Removed 356 rows containing missing values (geom_point).

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-85-1.png)<!-- -->

## The same data but aggregated by industry

``` r
farms_full <- read_csv(here("data", "raw data", "Farm structure EU",
                       "ef_kvftaa_1_Data_full.csv"),
                  na = c("", "NA", ":") # NA is coded as ":"
                  )
```

    ## 
    ## -- Column specification --------------------------------------------------------
    ## cols(
    ##   INDIC_EF = col_character(),
    ##   AGRAREA = col_character(),
    ##   GEO = col_character(),
    ##   FARMTYPE = col_character(),
    ##   TIME = col_double(),
    ##   Value = col_number(),
    ##   `Flag and Footnotes` = col_logical()
    ## )

drop columns we don’t need and rename ones we need

``` r
farms_full <- farms_full %>% 
  clean_names() 

farms_full %>% glimpse()
```

    ## Rows: 147,200
    ## Columns: 7
    ## $ indic_ef           <chr> "hold: Total number of holdings", "hold: Total n...
    ## $ agrarea            <chr> "Total", "Total", "Total", "Total", "Total", "To...
    ## $ geo                <chr> "Belgium", "Belgium", "Belgium", "Belgium", "Bel...
    ## $ farmtype           <chr> "Total", "Total", "Total", "Total", "Specialist ...
    ## $ time               <dbl> 2005, 2007, 2010, 2013, 2005, 2007, 2010, 2013, ...
    ## $ value              <dbl> 51540, 48010, 42850, 37760, 1510, 1500, 2030, 22...
    ## $ flag_and_footnotes <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...

The data set is a bit redundant filter only information we need

``` r
farms_full <- farms_full %>% 
  filter(indic_ef == "Euro: Standard output (SO)",
         farmtype != "Total") %>% 
  select(-flag_and_footnotes, -indic_ef) %>% 
  select(country = geo, agrarea, industry = farmtype,
         year = time, output_eur = value)

farms_full %>% glimpse()
```

    ## Rows: 28,160
    ## Columns: 5
    ## $ country    <chr> "Belgium", "Belgium", "Belgium", "Belgium", "Belgium", "...
    ## $ agrarea    <chr> "Total", "Total", "Total", "Total", "Total", "Total", "T...
    ## $ industry   <chr> "Specialist cereals, oilseed and protein crops (calculat...
    ## $ year       <dbl> 2005, 2007, 2010, 2013, 2005, 2007, 2010, 2013, 2005, 20...
    ## $ output_eur <dbl> 20898030, 21714800, 47082100, 64768630, 426695350, 43216...

We need to categorise industry to match the main dataset

``` r
farms_full %>% 
  distinct(industry) 
```

    ## # A tibble: 22 x 1
    ##    industry                                                                     
    ##    <chr>                                                                        
    ##  1 Specialist cereals, oilseed and protein crops (calculated with Standard Outp~
    ##  2 General field cropping (calculated with Standard Output)                     
    ##  3 Specialist horticulture indoor (calculated with Standard Output)             
    ##  4 Specialist horticulture outdoor (calculated with Standard Output)            
    ##  5 Other horticulture (calculated with Standard Output)                         
    ##  6 Specialist vineyards (calculated with Standard Output)                       
    ##  7 Specialist fruit and citrus fruit (calculated with Standard Output)          
    ##  8 Specialist olives (calculated with Standard Output)                          
    ##  9 Various permanent crops combined (calculated with Standard Output)           
    ## 10 Specialist dairying (calculated with Standard Output)                        
    ## # ... with 12 more rows

remove “(calculated with Standard Output)”

``` r
farms_full <- farms_full %>% 
  mutate(industry = str_remove(industry, pattern = " \\(.+\\)")) 
```

#### classify industry

Try to classify with the function we already created

``` r
farms_ind_rename <- farms_full %>% 
  select(industry) %>% 
  distinct() %>%
  mutate(new_value = map_chr(industry, ~ classify_industry(.x))) %>% 
  select(old_value = industry, new_value)

farms_ind_rename
```

    ## # A tibble: 22 x 2
    ##    old_value                                     new_value
    ##    <chr>                                         <chr>    
    ##  1 Specialist cereals, oilseed and protein crops cereals  
    ##  2 General field cropping                        other    
    ##  3 Specialist horticulture indoor                fruits   
    ##  4 Specialist horticulture outdoor               fruits   
    ##  5 Other horticulture                            fruits   
    ##  6 Specialist vineyards                          other    
    ##  7 Specialist fruit and citrus fruit             fruits   
    ##  8 Specialist olives                             other    
    ##  9 Various permanent crops combined              other    
    ## 10 Specialist dairying                           dairy    
    ## # ... with 12 more rows

Classified satisfactory

It will not be very efficient to categorize each of 28k values. Lets
write function which will map predefined value pairs

``` r
change_string_value <- function(target_value, old_values, new_values) {
  
  
  result <- NULL
  
  for (i in 1:length(old_values) ) {
    
    if (target_value == old_values[i]) {
      
      result <- new_values[i]
      
      break }
    
  }
  
  # if result is still NULL throw an error
  if ( is.null(result) ) stop("No matches, check old_values")
  
  return(result)
}
```

check function

``` r
farms_full %>% 
  select(industry) %>% 
  mutate(industry_cat = map_chr(farms_full$industry,
                                ~ change_string_value(target_value = .x,
                    old_values = farms_ind_rename$old_value,
                    new_values = farms_ind_rename$new_value))) %>%
  distinct() %>% 
  head(10)
```

    ## # A tibble: 10 x 2
    ##    industry                                      industry_cat
    ##    <chr>                                         <chr>       
    ##  1 Specialist cereals, oilseed and protein crops cereals     
    ##  2 General field cropping                        other       
    ##  3 Specialist horticulture indoor                fruits      
    ##  4 Specialist horticulture outdoor               fruits      
    ##  5 Other horticulture                            fruits      
    ##  6 Specialist vineyards                          other       
    ##  7 Specialist fruit and citrus fruit             fruits      
    ##  8 Specialist olives                             other       
    ##  9 Various permanent crops combined              other       
    ## 10 Specialist dairying                           dairy

Work well

classification function we created before works satisfactory

``` r
farms_full <- farms_full %>% 
  mutate(industry_cat = map_chr(farms_full$industry,
                                ~ change_string_value(target_value = .x,
                    old_values = farms_ind_rename$old_value,
                    new_values = farms_ind_rename$new_value))) %>% 
  select(-industry)
```

#### standardize country name

``` r
farm_country_rename <- farms_full %>% 
  select(old_country = country) %>% 
  distinct() %>% 
  mutate(new_country = map_chr(old_country, ~ standatise_country_name(.x)))

head(farm_country_rename)
```

    ## # A tibble: 6 x 2
    ##   old_country                                      new_country
    ##   <chr>                                            <chr>      
    ## 1 Belgium                                          Belgium    
    ## 2 Bulgaria                                         Bulgaria   
    ## 3 Czechia                                          Czechia    
    ## 4 Denmark                                          Denmark    
    ## 5 Germany (until 1990 former territory of the FRG) Germany    
    ## 6 Estonia                                          Estonia

``` r
farms_full <- farms_full %>% 
  mutate(country = map_chr(country,
                                ~ change_string_value(target_value = .x,
                                old_values = farm_country_rename$old_country,
                                new_values = farm_country_rename$new_country)))
```

#### change column position

``` r
farms_full <- farms_full %>% 
  select(country, industry_cat, year, everything())
```

### prepare for joining

``` r
farms_full_to_join <- farms_full %>%
  pivot_wider(names_from = agrarea, 
              values_from = output_eur,
              values_fn = sum) %>%      # to have a number in cell, it's list otherwise
  clean_names() %>% 
  group_by(country, year, industry_cat) %>%
  mutate(sfarms = sum(zero_ha, less_than_2_ha, from_2_to_4_9_ha,
                        from_5_to_9_9_ha, from_10_to_19_9_ha, na.rm = TRUE),
         sfarms_share = sfarms / total * 100) %>% 
  # add data for 2000 and 2018
  select(country,industry_cat, year, sfarms_share) %>% 
  pivot_wider(names_from = year, values_from = sfarms_share) %>%
  mutate(`2000` = `2005`, `2018` = `2013`, `1995` = `2005`) %>% 
  pivot_longer(cols = `2005`:`1995`,
               names_to = "year",
               values_to = "sfarm_share") %>% 
  mutate(year = as.numeric(year)) %>% 
  filter(!is.na(sfarm_share))

head(farms_full_to_join)
```

    ## # A tibble: 6 x 4
    ## # Groups:   country, industry_cat [1]
    ##   country industry_cat  year sfarm_share
    ##   <chr>   <chr>        <dbl>       <dbl>
    ## 1 Belgium cereals       2005        40.4
    ## 2 Belgium cereals       2007        39.7
    ## 3 Belgium cereals       2010        30.2
    ## 4 Belgium cereals       2013        30.9
    ## 5 Belgium cereals       2000        40.4
    ## 6 Belgium cereals       2018        30.9

``` r
ggplot(farms_full_to_join, aes(sfarm_share) ) +
  geom_density() +
  facet_grid(rows = vars(industry_cat))
```

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-99-1.png)<!-- -->

I didn\`t keep full list of industry categories so I will create it once
again

``` r
clean$industry_cat <- map_chr(clean$industry, ~ classify_industry(.x))
```

``` r
country_ind_year <-  country_year %>% 
  left_join(clean[c("id", "industry_cat")], by = "id" ) %>% 
  select(id, country, industry_cat, year)

head(country_ind_year)
```

    ## # A tibble: 6 x 4
    ##      id country industry_cat  year
    ##   <int> <chr>   <chr>        <dbl>
    ## 1     1 Germany meat          1995
    ## 2     1 Germany meat          1996
    ## 3     1 Germany meat          1997
    ## 4     1 Germany meat          1998
    ## 5     1 Germany meat          1999
    ## 6     1 Germany meat          2000

### 

``` r
sfarm_by_cat <- country_ind_year %>%
  left_join(farms_full_to_join, 
            by = c("country", "industry_cat", "year"),
            copy = TRUE) %>% 
  # calculate an average SfarmShare index attributed to each MPI observation
  group_by(id) %>% 
  summarise(sfarm_share_by_cat = mean(sfarm_share, na.rm = TRUE)) %>% 
  ungroup()

head(sfarm_by_cat)
```

    ## # A tibble: 6 x 2
    ##      id sfarm_share_by_cat
    ##   <int>              <dbl>
    ## 1     1               8.92
    ## 2     2               8.92
    ## 3     3               8.92
    ## 4     4               8.92
    ## 5     5             NaN   
    ## 6     6             NaN

### Finally join to main data set

``` r
clean <- clean %>% 
  inner_join(sfarm_by_cat, by = "id", copy = TRUE)
```

``` r
sum(is.na(clean$sfarm_share_by_cat))
```

    ## [1] 370

``` r
ggplot(clean, aes(sfarm_share_by_cat, mp_index, color = approach)) +
  geom_point(alpha = 0.4, size = 5) +
  theme_minimal()
```

    ## Warning: Removed 370 rows containing missing values (geom_point).

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-105-1.png)<!-- -->

## Add data from Doing Buisiness “easines to start business”

``` r
db <- readxl::read_excel(path = here("data", "raw data", "Doing Business.xlsx"),
                         sheet = 1, skip = 3)
```

    ## New names:
    ## * `Procedures (number)` -> `Procedures (number)...29`
    ## * `Score-Procedures (number)` -> `Score-Procedures (number)...30`
    ## * `Time (days)` -> `Time (days)...31`
    ## * `Score-Time (days)` -> `Score-Time (days)...32`
    ## * `Procedures (number)` -> `Procedures (number)...46`
    ## * ...

``` r
dim(db)
```

    ## [1] 3606  199

select only columns we need

``` r
db <- db %>%
  clean_names() %>% 
  select(country_code, year = db_year, score_starting_a_business)
```

``` r
head(db, 10)
```

    ## # A tibble: 10 x 3
    ##    country_code  year score_starting_a_business
    ##    <chr>        <dbl>                     <dbl>
    ##  1 AFG           2020                      92.0
    ##  2 AFG           2019                      92.0
    ##  3 AFG           2018                      82.6
    ##  4 AFG           2017                      90.4
    ##  5 AFG           2016                      90.5
    ##  6 AFG           2015                      91.0
    ##  7 AFG           2014                      91.6
    ##  8 AFG           2013                      88.3
    ##  9 AFG           2012                      87.9
    ## 10 AFG           2011                      87.8

To decrease the number of dropped observations we assume that there is
significant difference between 2004 and 2000 in Start Business index

``` r
db <- db %>% 
  pivot_wider(names_from = year, 
              values_from = score_starting_a_business) %>% 
  mutate(`2000` = `2004`) %>% 
  pivot_longer(cols = -country_code,
               names_to = "year",
               values_to = "score_starting_a_business") %>% 
  mutate(year = as.numeric(year))
```

Adjust country names

``` r
guess_field(db$country_code)
```

    ##              code percent_of_unique_matched
    ## genc3c     genc3c                  87.32394
    ## iso3c       iso3c                  87.32394
    ## wb             wb                  87.32394
    ## wb_api3c wb_api3c                  87.32394

``` r
db$country <- standatise_country_name(db$country_code,
                                      initial_code_scheme = "genc3c")
```

``` r
db %>% 
  select(country_code, country) %>% 
  distinct() %>% 
  head(10)
```

    ## # A tibble: 10 x 2
    ##    country_code country          
    ##    <chr>        <chr>            
    ##  1 AFG          Afghanistan      
    ##  2 ALB          Albania          
    ##  3 DZA          Algeria          
    ##  4 AGO          Angola           
    ##  5 ATG          Antigua & Barbuda
    ##  6 ARG          Argentina        
    ##  7 ARM          Armenia          
    ##  8 AUS          Australia        
    ##  9 AUT          Austria          
    ## 10 AZE          Azerbaijan

``` r
db_join <- country_year %>%
  left_join(db, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average DB index attributed to each MPI observation
  group_by(id) %>% 
  summarise(start_business = mean(score_starting_a_business, na.rm = TRUE)) %>% 
  ungroup()

head(db_join, 10)
```

    ## # A tibble: 10 x 2
    ##       id start_business
    ##    <int>          <dbl>
    ##  1     1           73.2
    ##  2     2           73.2
    ##  3     3           73.2
    ##  4     4           73.2
    ##  5     5          NaN  
    ##  6     6          NaN  
    ##  7     7          NaN  
    ##  8     8          NaN  
    ##  9     9          NaN  
    ## 10    10          NaN

``` r
sum(is.na(db_join$start_business))
```

    ## [1] 182

``` r
clean <- clean %>% 
  inner_join(db_join, by = "id", copy = TRUE)
```

``` r
ggplot(clean, aes(start_business, mp_index, color = approach)) +
  geom_point(alpha = 0.4, size = 5) +
  theme_minimal()
```

    ## Warning: Removed 182 rows containing missing values (geom_point).

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-116-1.png)<!-- -->

## Ease of doing business index

The algorithm is the same as with previous variable

``` r
db <- readxl::read_excel(path = here("data", "raw data", "Doing Business.xlsx"),
                         sheet = 1, skip = 3)
```

    ## New names:
    ## * `Procedures (number)` -> `Procedures (number)...29`
    ## * `Score-Procedures (number)` -> `Score-Procedures (number)...30`
    ## * `Time (days)` -> `Time (days)...31`
    ## * `Score-Time (days)` -> `Score-Time (days)...32`
    ## * `Procedures (number)` -> `Procedures (number)...46`
    ## * ...

``` r
db <- db %>%
  clean_names() %>% 
  select(country_code, year = db_year,
         starts_with("ease_of_doing_business_score")) %>% 
  group_by(country_code, year) %>% 
  # make one column with ease of doing business index
  # assume that there is no difference in methodologies
  mutate(edb_index = sum(ease_of_doing_business_score_db17_20_methodology,
                          ease_of_doing_business_score_db15_methodology,
                          ease_of_doing_business_score_db10_14_methodology,
                          na.rm = TRUE),
         country = standatise_country_name(country_code,
                                           initial_code_scheme = "genc3c")) %>%
  ungroup() %>% 
  select(country, year, edb_index) %>%
  # because on NAs some values are == 0
  # filter them out
  filter(edb_index > 0) %>% 
  # assume that there is no big difference in doing business in 2010 and 2005
  pivot_wider(names_from = year, 
            values_from = edb_index) %>% 
  mutate(`2005` = `2010`) %>% 
  pivot_longer(cols = -country,
             names_to = "year",
             values_to = "edb_index") %>% 
  mutate(year = as.numeric(year))

head(db)
```

    ## # A tibble: 6 x 3
    ##   country      year edb_index
    ##   <chr>       <dbl>     <dbl>
    ## 1 Afghanistan  2020      44.1
    ## 2 Afghanistan  2019      44.2
    ## 3 Afghanistan  2018      37.1
    ## 4 Afghanistan  2017      38.9
    ## 5 Afghanistan  2016      39.3
    ## 6 Afghanistan  2015      41.7

``` r
edb_join <- country_year %>%
  left_join(db, 
            by = c("country", "year"),
            copy = TRUE) %>% 
  # calculate an average DB index attributed to each MPI observation
  group_by(id) %>% 
  summarise(edb_index = mean(edb_index, na.rm = TRUE)) %>% 
  ungroup()
```

``` r
clean <- clean %>% 
  inner_join(edb_join, by = "id", copy = TRUE)
```

``` r
ggplot(clean, aes(edb_index, mp_index, color = approach)) +
  geom_point(alpha = 0.4, size = 5) +
  theme_minimal()
```

    ## Warning: Removed 291 rows containing missing values (geom_point).

![](1.-Data-Preparation_files/figure-gfm/unnamed-chunk-121-1.png)<!-- -->

## Make an object only with columns we need for the further analysis

``` r
glimpse(clean)
```

    ## Rows: 587
    ## Columns: 33
    ## $ id                 <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...
    ## $ year               <dbl> 2008, 2008, 2008, 2008, 1982, 1982, 1982, 1982, ...
    ## $ authors            <chr> "Anders, S. M", "Anders, S. M", "Anders, S. M", ...
    ## $ title              <chr> "Imperfect Competition in German Food Retailing:...
    ## $ country            <chr> "Germany", "Germany", "Germany", "Germany", "Uni...
    ## $ industry           <chr> "Beef", "Beef", "Pork", "Pork", "Textile", "Toba...
    ## $ approach           <fct> GIM, GIM, GIM, GIM, PTA, PTA, PTA, PTA, PTA, PTA...
    ## $ period             <chr> "1995-2000", "1995-2000", "1995-2000", "1995-200...
    ## $ obs_start          <int> 1995, 1995, 1995, 1995, 1947, 1947, 1947, 1947, ...
    ## $ obs_stop           <int> 2000, 2000, 2000, 2000, 1971, 1971, 1971, 1971, ...
    ## $ data_freq          <fct> Monthly, Monthly, Monthly, Monthly, Yearly, Year...
    ## $ mp_type            <fct> Oligopsony, Oligopoly, Oligopsony, Oligopoly, Ol...
    ## $ agg_level          <fct> Country, Country, Country, Country, Country, Cou...
    ## $ SC_level           <fct> Retail, Retail, Retail, Retail, Processor, Proce...
    ## $ LI                 <int> 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...
    ## $ CV                 <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, ...
    ## $ WOS                <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...
    ## $ mp_index           <dbl> 0.17600, 0.08900, 0.01100, 0.00300, 0.03684, 0.4...
    ## $ data_freq_fct      <fct> Monthly, Monthly, Monthly, Monthly, Yearly, Year...
    ## $ n_obs_per_year     <dbl> 12, 12, 12, 12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...
    ## $ industry_fct       <fct> meat, meat, meat, meat, other, tobacco, other, o...
    ## $ country_fct        <fct> europe, europe, europe, europe, n_america, n_ame...
    ## $ obs_years          <int> 5, 5, 5, 5, 24, 24, 24, 24, 24, 24, 22, 23, 23, ...
    ## $ n_of_obs           <dbl> 60, 60, 60, 60, 24, 24, 24, 24, 24, 24, 22, 23, ...
    ## $ after_2005         <dbl> 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...
    ## $ perish             <int> 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...
    ## $ mean_pse           <dbl> 37.73444, 37.73444, 37.73444, 37.73444, NaN, NaN...
    ## $ mean_pp            <dbl> 1.352539, 1.352539, 1.352539, 1.352539, NaN, NaN...
    ## $ sfarm_share        <dbl> 19.69004, 19.69004, 19.69004, 19.69004, NaN, NaN...
    ## $ industry_cat       <chr> "meat", "meat", "meat", "meat", "other", "tobacc...
    ## $ sfarm_share_by_cat <dbl> 8.917172, 8.917172, 8.917172, 8.917172, NaN, NaN...
    ## $ start_business     <dbl> 73.24874, 73.24874, 73.24874, 73.24874, NaN, NaN...
    ## $ edb_index          <dbl> NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN...

``` r
drop_col <- c("id", "year", "authors", "title", "country", "industry", "period",
              "obs_start", "obs_stop", "n_obs_per_year",
              "obs_years", "industry_cat", "data_freq_fct")

analysis <- clean %>%
  # select all except of drop_col
  select(-all_of(drop_col)) %>% 
  rename(app = approach,
         freq = data_freq,
         type = mp_type,
         index = mp_index,
         ind = industry_fct,
         agg = agg_level,
         SC = SC_level,
         count = country_fct,
         obs_n = n_of_obs) %>%
  # create dummies from category columns
  fastDummies::dummy_columns(select_columns = c("freq", "type", "ind",
                                                "count", "app", "agg", "SC"),
                remove_selected_columns = TRUE)
```

``` r
glimpse(analysis)
```

    ## Rows: 587
    ## Columns: 44
    ## $ LI                 <int> 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...
    ## $ CV                 <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, ...
    ## $ WOS                <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...
    ## $ index              <dbl> 0.17600, 0.08900, 0.01100, 0.00300, 0.03684, 0.4...
    ## $ obs_n              <dbl> 60, 60, 60, 60, 24, 24, 24, 24, 24, 24, 22, 23, ...
    ## $ after_2005         <dbl> 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...
    ## $ perish             <int> 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...
    ## $ mean_pse           <dbl> 37.73444, 37.73444, 37.73444, 37.73444, NaN, NaN...
    ## $ mean_pp            <dbl> 1.352539, 1.352539, 1.352539, 1.352539, NaN, NaN...
    ## $ sfarm_share        <dbl> 19.69004, 19.69004, 19.69004, 19.69004, NaN, NaN...
    ## $ sfarm_share_by_cat <dbl> 8.917172, 8.917172, 8.917172, 8.917172, NaN, NaN...
    ## $ start_business     <dbl> 73.24874, 73.24874, 73.24874, 73.24874, NaN, NaN...
    ## $ edb_index          <dbl> NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN...
    ## $ freq_Daily         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ freq_Monthly       <int> 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...
    ## $ freq_Quarterly     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ freq_Weekly        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ freq_Yearly        <int> 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...
    ## $ type_Oligopsony    <int> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, ...
    ## $ type_Oligopoly     <int> 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, ...
    ## $ ind_meat           <int> 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...
    ## $ ind_other          <int> 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...
    ## $ ind_tobacco        <int> 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ ind_paper          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ ind_beverages      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ ind_dairy          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ ind_fruits         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ ind_sugar          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ ind_oils           <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ ind_cereals        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ ind_bread_or_flour <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ count_europe       <int> 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...
    ## $ count_n_america    <int> 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...
    ## $ count_other        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ app_GIM            <int> 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ app_PTA            <int> 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
    ## $ app_Other          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ app_SFA            <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ agg_Country        <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
    ## $ agg_Region         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ agg_Firm           <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ agg_World          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ SC_Retail          <int> 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    ## $ SC_Processor       <int> 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...

``` r
saveRDS(clean, file = here("data", "clean data", "Full_data.Rds"))
saveRDS(analysis, file = here("data", "clean data", "Analysis_data.Rds"))
```
